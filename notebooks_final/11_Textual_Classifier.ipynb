{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcf8791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DistilBertConfig, DistilBertModel, DistilBertTokenizerFast\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import accelerate\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df6787",
   "metadata": {},
   "source": [
    "### UTILS CLASS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8445ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Label(Enum):\n",
    "    DOG = \"dog\"\n",
    "    BIKE = \"bike\"\n",
    "    BALL = \"ball\"\n",
    "    WATER = \"water\"    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7943068",
   "metadata": {},
   "source": [
    "### UTILS METHODS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9fcfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(filename: str):\n",
    "    return filename.split(\"_\")[0]\n",
    "\n",
    "\n",
    "def get_uuid(filename: str):\n",
    "    name = Path(filename).stem          \n",
    "    parts = name.split(\"_\")\n",
    "    return \"_\".join(parts[:2])          \n",
    "\n",
    "\n",
    "def build_augmented_path(img_path: Path, base_dir: Path):\n",
    "    img_path = Path(img_path)\n",
    "    filename = img_path.name\n",
    "    label = get_label(filename)\n",
    "    uuid = get_uuid(filename)\n",
    "    print(uuid)\n",
    "\n",
    "    return base_dir / label / uuid / filename\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addebeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(lbl):\n",
    "    if isinstance(lbl, Label):\n",
    "        return lbl.value \n",
    "    if isinstance(lbl, str) and lbl.startswith(\"Label.\"):\n",
    "        return lbl.split(\".\")[1].lower()  \n",
    "    return lbl.lower()  \n",
    "\n",
    "def encode_labels(labels, class_to_idx):\n",
    "    cleaned = [normalize_label(lbl) for lbl in labels]\n",
    "    return np.array([class_to_idx[lbl] for lbl in cleaned], dtype=np.int64)\n",
    "\n",
    "classes = [lbl.value for lbl in Label]  \n",
    "class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "print(class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9198df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_oov_pct(sequences, oov_id):\n",
    "    total = sum(1 for seq in sequences for tid in seq if tid != 0)\n",
    "    oov = sum(1 for seq in sequences for tid in seq if tid == oov_id)\n",
    "    return oov/total*100 if total > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a3a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_label(lbl):\n",
    "    if hasattr(lbl, \"value\"):  # Enum Label\n",
    "        return str(lbl.value).lower()\n",
    "    s = str(lbl)\n",
    "    if s.startswith(\"Label.\"):\n",
    "        return s.split(\".\", 1)[1].lower()\n",
    "    return s.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0453cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels, mapping):\n",
    "    cleaned = [normalize_label(lbl) for lbl in labels]\n",
    "    return np.array([mapping[c] for c in cleaned], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a8ca65",
   "metadata": {},
   "source": [
    "### PREPROCESSING METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8308d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_padded_ids(texts, tok, max_vocab, seq_len, pad_id=0, oov_id=1):\n",
    "    seqs = tok.texts_to_sequences(texts)\n",
    "\n",
    "    # Force les ids dans [0..max_vocab-1], sinon -> OOV\n",
    "    seqs = [[t if (0 <= t < max_vocab) else oov_id for t in s] for s in seqs]\n",
    "\n",
    "    padded = pad_sequences(\n",
    "        seqs,\n",
    "        maxlen=seq_len,\n",
    "        padding=\"post\",\n",
    "        truncating=\"post\",\n",
    "        value=pad_id\n",
    "    ).astype(np.int64)\n",
    "\n",
    "    return padded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760fa84b",
   "metadata": {},
   "source": [
    "### SMALLBERT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4019dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, sequence_length: int, vocab_size:int, embed_dim:int):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(sequence_length, embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len = x.size()\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        return self.token_embeddings(x) + self.position_embeddings(positions)\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout_rate: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout_rate, batch_first=True)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, padding_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        attn_output, _ = self.att(\n",
    "            x, x, x,\n",
    "            key_padding_mask=padding_mask  # <-- correct masking\n",
    "        )\n",
    "\n",
    "        x = self.layernorm1(x + self.dropout1(attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        out = self.layernorm2(x + self.dropout2(ffn_output))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7317e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallBERT(nn.Module):\n",
    "    def __init__(self, sequence_length: int, vocab_size: int, embed_dim: int,\n",
    "                 num_heads: int, ff_dim: int, num_layers: int) -> None:\n",
    "        super().__init__()\n",
    "        self.pos_embedding = PositionalEmbedding(sequence_length, vocab_size, embed_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len) token ids\n",
    "        \"\"\"\n",
    "        padding_mask = (x == 0)  #0= PAD\n",
    "        x = self.pos_embedding(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, padding_mask=padding_mask)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallBERTPourClassification(nn.Module):\n",
    "    def __init__(self, sequence_length: int, vocab_size: int, embed_dim: int,\n",
    "                 num_heads: int, ff_dim: int, num_layers: int,\n",
    "                 num_classes: int = 4) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = SmallBERT(sequence_length, vocab_size, embed_dim, num_heads, ff_dim, num_layers)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        enc = self.encoder(x)             # (batch, seq_len, embed_dim)\n",
    "        pooled = enc.mean(dim=1)          #mean pooling\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits  # logits only, pas softmax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69cd6fc",
   "metadata": {},
   "source": [
    "### TRAINING METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a00671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.should_stop = False\n",
    "\n",
    "    def step(self, metric):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = metric\n",
    "        elif metric < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "        else:\n",
    "            self.best_score = metric\n",
    "            self.counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25665ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(y.cpu().numpy())\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e470c61",
   "metadata": {},
   "source": [
    "### VISUALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "def plot_losses(train_losses, test_losses):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label=\"Train loss\")\n",
    "    plt.plot(test_losses, label=\"Test loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Test Loss\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def plot_losses(train_losses, test_losses):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label=\"Train loss\")\n",
    "    plt.plot(test_losses, label=\"Test loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Test Loss\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names=None, title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\", xticks_rotation=45)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_multiclass_roc(y_true, y_prob, num_classes, class_names=None, title=\"ROC (One-vs-Rest)\"):\n",
    "    \"\"\"\n",
    "    y_true: [N] int labels\n",
    "    y_prob: [N, C] probabilities\n",
    "    \"\"\"\n",
    "    y_true_bin = label_binarize(y_true, classes=list(range(num_classes)))  # [N, C]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for c in range(num_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin[:, c], y_prob[:, c])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        name = class_names[c] if class_names is not None else f\"Class {c}\"\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC={roc_auc:.3f})\")\n",
    "\n",
    "    # Diagonal\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5347e45",
   "metadata": {},
   "source": [
    "### DISTILLBERT MODEL CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8de27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Retourne un batch standardisé:\n",
    "    - idx: index dans le dataset\n",
    "    - input_ids: (L,)\n",
    "    - attention_mask: (L,)\n",
    "    - labels: ( )\n",
    "    - text: str (optionnel, utile pour debug)\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128, return_text=True):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = np.asarray(labels, dtype=np.int64)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.return_text = return_text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {\n",
    "            \"idx\": torch.tensor(idx, dtype=torch.long),\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0).to(torch.long),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0).to(torch.long),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }\n",
    "        if self.return_text:\n",
    "            item[\"text\"] = text\n",
    "        return item\n",
    "\n",
    "def make_text_loaders(X_train, y_train, X_val, y_val, X_test, y_test,\n",
    "                      model_name=\"distilbert-base-uncased\", max_len=128,\n",
    "                      batch_size=32):\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "\n",
    "    train_ds = TextClsDataset(X_train, y_train, tokenizer, max_len=max_len)\n",
    "    val_ds   = TextClsDataset(X_val,   y_val,   tokenizer, max_len=max_len)\n",
    "    test_ds  = TextClsDataset(X_test,  y_test,  tokenizer, max_len=max_len)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return tokenizer, train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f56b78",
   "metadata": {},
   "source": [
    "### DISTILLBERT TRAINING METHODS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c3141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device_batch(batch, device):\n",
    "    out = {}\n",
    "    for k, v in batch.items():\n",
    "        if torch.is_tensor(v):\n",
    "            out[k] = v.to(device, non_blocking=True)\n",
    "        else:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "def forward_text(model, batch, arch=\"hf\"):\n",
    "    \"\"\"\n",
    "    essai de méthode générique\n",
    "    archicture:\n",
    "      - \"hf\": DistilBERT/Transformers => model(**) retourne un objet avec .logits\n",
    "      - \"smallbert\": ton modèle => model(input_ids) -> logits\n",
    "    \"\"\"\n",
    "    if arch == \"hf\":\n",
    "        out = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "        logits = out.logits\n",
    "        return logits\n",
    "    elif arch == \"smallbert\":\n",
    "        # ton SmallBERT doit accepter input_ids (LongTensor)\n",
    "        logits = model(batch[\"input_ids\"])\n",
    "        return logits\n",
    "    else:\n",
    "        raise ValueError(f\"arch inconnu: {arch}\")\n",
    "\n",
    "def train_one_epoch_text(model, loader, optimizer, criterion, device, epoch, epochs, arch=\"hf\"):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    loop = tqdm(loader, total=len(loader), desc=f\"Epoch {epoch+1}/{epochs} [TRAIN]\")\n",
    "    for i, batch in enumerate(loop):\n",
    "        batch = to_device_batch(batch, device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        logits = forward_text(model, batch, arch=arch)\n",
    "        loss = criterion(logits, batch[\"labels\"])\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        loop.set_postfix(loss=running_loss / (i + 1))\n",
    "\n",
    "    return running_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_text(model, loader, criterion, device, epoch, epochs, num_classes, arch=\"hf\",\n",
    "                  class_names=None, n_mistakes_to_print=5):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    all_labels, all_preds, all_probs = [], [], []\n",
    "    mistakes_printed = 0\n",
    "\n",
    "    loop = tqdm(loader, total=len(loader), desc=f\"Epoch {epoch+1}/{epochs} [EVAL]\")\n",
    "    for batch in loop:\n",
    "        batch = to_device_batch(batch, device)\n",
    "\n",
    "        logits = forward_text(model, batch, arch=arch)\n",
    "        loss = criterion(logits, batch[\"labels\"])\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        preds = probs.argmax(dim=1)\n",
    "\n",
    "\n",
    "        all_labels.append(batch[\"labels\"].detach().cpu())\n",
    "        all_preds.append(preds.detach().cpu())\n",
    "        all_probs.append(probs.detach().cpu())\n",
    "\n",
    "        # debug erreurs (optionnel)\n",
    "        if (\"text\" in batch) and mistakes_printed < n_mistakes_to_print:\n",
    "            mism = (preds != batch[\"labels\"]).detach().cpu()\n",
    "            if mism.any():\n",
    "                pos_list = torch.where(mism)[0].tolist()\n",
    "                for pos in pos_list:\n",
    "                    if mistakes_printed >= n_mistakes_to_print:\n",
    "                        break\n",
    "                    true_i = int(batch[\"labels\"][pos].item())\n",
    "                    pred_i = int(preds[pos].item())\n",
    "                    t = batch[\"text\"][pos]  # str\n",
    "                    tn = class_names[true_i] if class_names else str(true_i)\n",
    "                    pn = class_names[pred_i] if class_names else str(pred_i)\n",
    "                    print(\"\\n--- Mauvaise prédiction (texte) ---\")\n",
    "                    print(f\"true: {tn} ({true_i}) | pred: {pn} ({pred_i})\")\n",
    "                    print(f\"text: {t[:300]}{'...' if len(t) > 300 else ''}\")\n",
    "                    mistakes_printed += 1\n",
    "\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    y_true = torch.cat(all_labels).numpy()\n",
    "    y_pred = torch.cat(all_preds).numpy()\n",
    "    y_prob = torch.cat(all_probs).numpy()\n",
    "    acc = float((torch.tensor(y_pred) == torch.tensor(y_true)).float().mean().item())\n",
    "\n",
    "    return avg_loss, y_true, y_pred, y_prob, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589adcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(path: Path) -> None:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def fit_text(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    num_classes=4,\n",
    "    epochs=10,\n",
    "    lr=2e-5,\n",
    "    patience=3,\n",
    "    models_dir=Path(\"../models\"),\n",
    "    best_name=\"best_text_model.pth\",\n",
    "    criterion=None,\n",
    "    class_names=None,\n",
    "    arch=\"hf\",                 # \"hf\"  \"smallbert\"\n",
    "    optimizer_name=\"adamw\"     # \"adamw\"  \"sgd\"\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    if criterion is None:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if optimizer_name.lower() == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    else:\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    ensure_dir(models_dir)\n",
    "    best_model_path = models_dir / best_name\n",
    "\n",
    "    best_test_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_losses, test_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_one_epoch_text(\n",
    "            model, train_loader, optimizer, criterion, device, epoch, epochs, arch=arch\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        print(f\"\\nEpoch {epoch+1} - Average TRAIN loss: {train_loss:.4f}\")\n",
    "\n",
    "        test_loss, y_true, y_pred, y_prob, acc = evaluate_text(\n",
    "            model, test_loader, criterion, device, epoch, epochs, num_classes=num_classes,\n",
    "            arch=arch, class_names=class_names, n_mistakes_to_print=0\n",
    "        )\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Average TEST loss: {test_loss:.4f} | acc={acc:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_true, y_pred, digits=3, target_names=class_names))\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Nouveau meilleur modèle sauvegardé (test loss: {best_test_loss:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Pas d'amélioration ({patience_counter}/{patience})\")\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping déclenché après {epoch+1} époques\")\n",
    "                print(f\"Meilleur test loss: {best_test_loss:.4f}\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\nChargement du meilleur modèle (test loss: {best_test_loss:.4f})\")\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "\n",
    "    final_test_loss, y_true, y_pred, y_prob, acc = evaluate_text(\n",
    "        model, test_loader, criterion, device, epoch=0, epochs=1, \n",
    "        arch=arch, class_names=class_names, n_mistakes_to_print=5, num_classes=4\n",
    "    )\n",
    "    print(f\"Best model - TEST loss: {final_test_loss:.4f} | acc={acc:.4f}\")\n",
    "\n",
    "    # Plots (tu peux réutiliser tes fonctions inchangées)\n",
    "    plot_losses(train_losses, test_losses)\n",
    "    plot_confusion_matrix(y_true, y_pred, class_names=class_names, title=\"Confusion Matrix (Best Model)\")\n",
    "    plot_multiclass_roc(y_true, y_prob, num_classes=num_classes, class_names=class_names, title=\"ROC (Best Model, OvR)\")\n",
    "\n",
    "    return model, {\"train_losses\": train_losses, \"test_losses\": test_losses, \"best_test_loss\": best_test_loss}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3130a110",
   "metadata": {},
   "source": [
    "### DATASET LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64422b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = Path(\"../data/final_dataset/metadata.csv\")\n",
    "df = pd.read_csv(metadata_path)\n",
    "print(df.columns)\n",
    "print(df.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.drop_duplicates(subset=\"caption\").reset_index(drop=True)\n",
    "\n",
    "df_train, df_temp = train_test_split(df_clean, test_size=0.3, random_state=11)\n",
    "df_test, df_val = train_test_split(df_temp, test_size=0.5, random_state=11)\n",
    "\n",
    "print(df_train[\"label\"].value_counts(normalize=True) * 100)\n",
    "print(df_val[\"label\"].value_counts(normalize=True) * 100)\n",
    "print(df_test[\"label\"].value_counts(normalize=True) * 100)\n",
    "\n",
    "X_train, y_train, caption_train = df_train[\"image_path\"], df_train[\"label\"], df_train[\"caption\"]\n",
    "X_val, y_val, caption_val  = df_val[\"image_path\"], df_val[\"label\"], df_val[\"caption\"]\n",
    "X_test, y_test, caption_test   = df_test[\"image_path\"], df_test[\"label\"], df_test[\"caption\"]\n",
    "\n",
    "X_train = caption_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_val = caption_val.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "X_test = caption_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "print(\"Avant :\", len(df))\n",
    "print(\"Après  :\", len(df_clean))\n",
    "print(\"Doublons supprimés :\", len(df) - len(df_clean))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f938659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"caption\"])\n",
    "actual_vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"   taille de vocabulaire: {len(tokenizer.word_index)}\")\n",
    "print(f\"   vocab_size: {actual_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d91af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42a40f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])\n",
    "print(X_train_seq[0])\n",
    "print(X_test[0])\n",
    "print(X_test_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daf3660",
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_id = tokenizer.word_index[\"<OOV>\"]\n",
    "\n",
    "count = 0\n",
    "for i in range(len(X_val_seq)):\n",
    "    count += X_val_seq[i].count(oov_id)\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_seq_len, padding='post', truncating='post')\n",
    "X_val_padded = pad_sequences(X_val_seq, maxlen=max_seq_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_seq_len, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0b34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB = 1000     # tokens autorisés: 0..999\n",
    "PAD_ID = 0\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "SEQ_LEN = 104 \n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=OOV_TOKEN)\n",
    "tokenizer.fit_on_texts(df_clean[\"caption\"])\n",
    "\n",
    "oov_id = tokenizer.word_index[OOV_TOKEN]\n",
    "print(\"oov_id:\", oov_id)\n",
    "\n",
    "X_train_padded = texts_to_padded_ids(X_train, tokenizer, MAX_VOCAB, SEQ_LEN, PAD_ID, oov_id)\n",
    "X_val_padded   = texts_to_padded_ids(X_val,   tokenizer, MAX_VOCAB, SEQ_LEN, PAD_ID, oov_id)\n",
    "X_test_padded  = texts_to_padded_ids(X_test,  tokenizer, MAX_VOCAB, SEQ_LEN, PAD_ID, oov_id)\n",
    "\n",
    "word_counts = tokenizer.word_counts\n",
    "total = sum(word_counts.values())\n",
    "\n",
    "def coverage(k):\n",
    "    topk = sorted(word_counts.values(), reverse=True)[:k]\n",
    "    return sum(topk) / total\n",
    "\n",
    "for k in [1000, 2000, 3000, 5000]:\n",
    "    print(k, f\"{coverage(k)*100:.1f}%\")\n",
    " \n",
    "classes = [lbl.value for lbl in Label]\n",
    "classes = [c.lower() for c in classes]\n",
    "class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "print(\"class_to_idx:\", class_to_idx)\n",
    "\n",
    "def normalize_text(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(r\"\\d+\", \" \", t)                 # supprime les nombres\n",
    "    t = re.sub(r\"[^\\w\\s]\", \" \", t)             # ponctuation\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "    words = [lemmatizer.lemmatize(w) for w in t.split()]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "df_clean[\"caption\"] = df_clean[\"caption\"].apply(normalize_text)\n",
    "df_clean[\"caption\"].iloc[1]\n",
    "\n",
    "lengths = df_clean[\"caption\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(lengths.describe())\n",
    "\n",
    "def normalize_label(lbl):\n",
    "    s = str(lbl)\n",
    "    if s.startswith(\"Label.\"):\n",
    "        s = s.split(\".\", 1)[1]\n",
    "    return s.lower()\n",
    "\n",
    "df_clean[\"label\"] = df_clean[\"label\"].apply(normalize_label)\n",
    "\n",
    "p95 = int(lengths.quantile(0.95))\n",
    "p98 = int(lengths.quantile(0.98))\n",
    "max_len = int(lengths.max())\n",
    "\n",
    "print(f\"p95={p95}, p98={p98}, max={max_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7c1f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train_np = encode_labels(y_train, class_to_idx)\n",
    "y_val_np   = encode_labels(y_val, class_to_idx)\n",
    "y_test_np  = encode_labels(y_test, class_to_idx)\n",
    "num_classes = len(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600358f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_id = tokenizer.word_index.get(\"<OOV>\", 1)\n",
    "\n",
    "print(f\"\\n<OOV>pourcentage:\")\n",
    "print(f\"  Train: {count_oov_pct(X_train_seq, oov_id):.1f}%\")\n",
    "print(f\"  Val:   {count_oov_pct(X_val_seq, oov_id):.1f}%\")\n",
    "print(f\"  Test:  {count_oov_pct(X_test_seq, oov_id):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca0979",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.tensor(X_train_padded, dtype=torch.long)\n",
    "X_val_t   = torch.tensor(X_val_padded,   dtype=torch.long)\n",
    "X_test_t  = torch.tensor(X_test_padded,  dtype=torch.long)\n",
    "\n",
    "y_train_t = torch.tensor(y_train_np, dtype=torch.long)\n",
    "y_val_t   = torch.tensor(y_val_np,   dtype=torch.long)\n",
    "y_test_t  = torch.tensor(y_test_np,  dtype=torch.long)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val_t,   y_val_t),   batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(TensorDataset(X_test_t,  y_test_t),  batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456fe829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = TensorDataset(torch.tensor(X_train_padded), torch.tensor(y_train_np))\n",
    "# val_ds   = TensorDataset(torch.tensor(X_val_padded), torch.tensor(y_val_np))\n",
    "# test_ds  = TensorDataset(torch.tensor(X_test_padded), torch.tensor(y_test_np))\n",
    "\n",
    "# train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "# val_loader   = DataLoader(val_ds, batch_size=32)\n",
    "# test_loader  = DataLoader(test_ds, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82c13cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a79f8a39",
   "metadata": {},
   "source": [
    "### SMALLBERT TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433f8658",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MAX_VOCAB = 1000      \n",
    "SEQ_LEN = 128         \n",
    "NUM_CLASSES = 4\n",
    "\n",
    "model = SmallBERTPourClassification(\n",
    "    vocab_size=MAX_VOCAB,      \n",
    "    sequence_length=SEQ_LEN,   \n",
    "    embed_dim=128,\n",
    "    num_heads=4,\n",
    "    ff_dim=256,\n",
    "    num_layers=2,\n",
    "    num_classes=NUM_CLASSES\n",
    ").to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "all_probs = []\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        x = x.to(device)\n",
    "        enc = model.encoder(x)  #(batch, seq_len, embed_dim)\n",
    "        emb = enc.mean(dim=1).cpu().numpy()  # (batch, emd_dim)\n",
    "\n",
    "        embeddings.append(emb)\n",
    "        labels.extend(y.numpy())\n",
    "\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=-1)  \n",
    "        pred = probs.argmax(dim=1)\n",
    "        \n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "        all_labels.extend(y.numpy())\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "labels = np.array(labels)\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, learning_rate=\"auto\", init=\"pca\")\n",
    "emb2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(emb2d[:,0], emb2d[:,1], c=labels, cmap=\"tab10\", s=12)\n",
    "plt.title(\"t-SNE sur embeddings (avant entraînement)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a67fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "early = EarlyStopping(patience=10)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "models_dir = Path(\"../models\")\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "best_model_path = models_dir / \"best-model-smallbert.pth\"\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_true = [], []\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            val_loss += loss.item()\n",
    "            pred = logits.argmax(dim=1)\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_true.extend(y.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    val_acc = accuracy_score(all_true, all_preds)\n",
    "    val_f1 = f1_score(all_true, all_preds, average=\"macro\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | train_loss={avg_train_loss:.4f} | val_loss={avg_val_loss:.4f} | acc={val_acc:.4f} | f1={val_f1:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\" Meilleur modèle sauvegardé (val_loss: {best_val_loss:.4f})\")\n",
    "\n",
    "    early.step(val_f1)\n",
    "    if early.should_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e8f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nChargement du meilleur modèle (val_loss: {best_val_loss:.4f})\")\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label=\"Train loss\")\n",
    "plt.plot(val_losses, label=\"Validation loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss - SmallBERT\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_text(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    arch=\"smallbert\",\n",
    "    class_names=[\"dog\", \"bike\", \"ball\", \"water\"],\n",
    "    num_classes=4,\n",
    "    lr=1e-4,\n",
    "    epochs=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1f5de6",
   "metadata": {},
   "source": [
    "### SMALLBERT EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0c03ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "all_probs = []\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in val_loader:\n",
    "        x = x.to(device)\n",
    "        enc = model.encoder(x)  #(batch, seq_len, embed_dim)\n",
    "        emb = enc.mean(dim=1).cpu().numpy()  # (batch, embed_dim)\n",
    "\n",
    "        embeddings.append(emb)\n",
    "        labels.extend(y.numpy())\n",
    "\n",
    "        logits = model(x)\n",
    "        probs = F.softmax(logits, dim=-1)  \n",
    "        pred = probs.argmax(dim=1)\n",
    "        \n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "        all_labels.extend(y.numpy())\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "labels = np.array(labels)\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "test_acc = accuracy_score(all_labels, all_preds)\n",
    "test_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "class_names = [idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"Test F1:\", test_f1)\n",
    "#CONFUSION\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "disp.plot(cmap=\"Blues\", values_format=\"d\", ax=ax, xticks_rotation=45)\n",
    "plt.title(\"Confusion Matrix - SmallBERT\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#ROC\n",
    "num_classes = len(class_names)\n",
    "y_true_bin = label_binarize(all_labels, classes=list(range(num_classes)))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for c in range(num_classes):\n",
    "    fpr, tpr, _ = roc_curve(y_true_bin[:, c], all_probs[:, c])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{class_names[c]} (AUC={roc_auc:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (One-vs-Rest) - SmallBERT\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e1f862",
   "metadata": {},
   "source": [
    "### DISTILLBERT TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"  # ou multilingual si captions FR\n",
    "tokenizer, train_loader, val_loader, test_loader = make_text_loaders(\n",
    "    X_train, y_train_np, X_val, y_val_np, X_test, y_test_np,\n",
    "    model_name=MODEL_NAME, max_len=128, batch_size=32\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=4)\n",
    "\n",
    "class_names = [\"dog\", \"bike\", \"ball\", \"water\"]\n",
    "model, logs = fit_text(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    num_classes=4,\n",
    "    epochs=5,\n",
    "    lr=2e-5,\n",
    "    patience=2,\n",
    "    best_name=\"best_distilbert.pth\",\n",
    "    class_names=class_names,\n",
    "    arch=\"hf\",\n",
    "    optimizer_name=\"adamw\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7758f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\" \n",
    "tokenizer, train_loader, val_loader, test_loader = make_text_loaders(\n",
    "    X_train, y_train_np, X_val, y_val_np, X_test, y_test_np,\n",
    "    model_name=MODEL_NAME, max_len=128, batch_size=32\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=4)\n",
    "\n",
    "class_names = [\"dog\", \"bike\", \"ball\", \"water\"]\n",
    "model, logs = fit_text(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    num_classes=4,\n",
    "    epochs=5,\n",
    "    lr=2e-5,\n",
    "    patience=2,\n",
    "    best_name=\"best_distilbert.pth\",\n",
    "    class_names=class_names,\n",
    "    arch=\"hf\",\n",
    "    optimizer_name=\"adamw\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
