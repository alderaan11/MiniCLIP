{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2ff1dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/MiniCLIP/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import io\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, List, Optional, Any\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Tuple\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import accelerate\n",
    "import transformers\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "from transformers import DistilBertConfig, DistilBertModel, DistilBertTokenizerFast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe89104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetVisionEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=256):\n",
    "        super().__init__()\n",
    "        base = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        self.backbone = nn.Sequential(\n",
    "            base.conv1, base.bn1, base.relu, base.maxpool,\n",
    "            base.layer1, base.layer2, base.layer3, base.layer4\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.projection = nn.Linear(base.fc.in_features, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.projection(x)       # (batch, embed_dim)\n",
    "        x = F.normalize(x, dim=-1)   # CLIP normalisation L2\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4d6b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, sequence_length: int, vocab_size:int, embed_dim:int):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(sequence_length, embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len = x.size()\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        return self.token_embeddings(x) + self.position_embeddings(positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cd7d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout_rate: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout_rate, batch_first=True)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, padding_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        attn_output, _ = self.att(\n",
    "            x, x, x,\n",
    "            key_padding_mask=padding_mask  # <-- correct masking\n",
    "        )\n",
    "\n",
    "        x = self.layernorm1(x + self.dropout1(attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        out = self.layernorm2(x + self.dropout2(ffn_output))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce8fbc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallBERT(nn.Module):\n",
    "    def __init__(self, sequence_length: int, vocab_size: int, embed_dim: int,\n",
    "                 num_heads: int, ff_dim: int, num_layers: int) -> None:\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.pos_embedding: PositionalEmbedding = PositionalEmbedding(sequence_length, vocab_size, embed_dim)\n",
    "        self.blocks: nn.ModuleList = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.layernorm: nn.LayerNorm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout: nn.Dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len) token ids\n",
    "        return: (batch, seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        x = self.pos_embedding(x)\n",
    "        mask: torch.Tensor = (x == 0)[:, :, 0]  # PAD mask\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, num_heads, ff_dim, num_layers, out_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = SmallBERT(sequence_length, vocab_size, embed_dim, num_heads, ff_dim, num_layers)\n",
    "        self.projection = nn.Linear(embed_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)           # (batch, seq_len, embed_dim)\n",
    "        pooled = enc.mean(dim=1)        # (batch, embed_dim)\n",
    "        z = self.projection(pooled)     # (batch, out_dim)\n",
    "        z = F.normalize(z, dim=-1)      # important pour CLIP\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81bd56d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCLIP(nn.Module):\n",
    "    def __init__(self, vision_encoder: nn.Module, text_encoder: nn.Module, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.vision = vision_encoder\n",
    "        self.text = text_encoder\n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature))\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        img_emb = self.vision(images)     # (batch, d)\n",
    "        txt_emb = self.text(captions)     # (batch, d)\n",
    "\n",
    "        logits = img_emb @ txt_emb.T       # similarité cosinus * car embeddings normalisés\n",
    "        logits = logits / self.temperature\n",
    "\n",
    "        return logits, img_emb, txt_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_loss(logits):\n",
    "    batch = logits.size(0)\n",
    "    labels = torch.arange(batch, device=logits.device)\n",
    "\n",
    "    loss_img = F.cross_entropy(logits, labels)\n",
    "    loss_txt = F.cross_entropy(logits.T, labels)\n",
    "\n",
    "    return (loss_img + loss_txt) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d89daa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(Dataset):\n",
    "    def __init__(self, df, base_dir: Path, tokenizer, image_transform, max_length=32):\n",
    "        self.img_paths = df[\"image_path\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.captions = df[\"caption\"].tolist()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_transform = image_transform\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # mapping constant\n",
    "        self.class_to_idx = {cls: i for i in sorted(set(self.labels))}\n",
    "\n",
    "        self.base_dir = base_dir\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ----- image -----\n",
    "        img_path = Path(build_augmented_path(self.img_paths[idx], self.base_dir))\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = self.image_transform(img)\n",
    "\n",
    "        # ----- text -----\n",
    "        caption = self.captions[idx]\n",
    "        enc = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        label = self.class_to_idx[self.labels[idx]]\n",
    "\n",
    "        return {\n",
    "            \"index\": idx,\n",
    "            \"image\": img,\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ee3dbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_path', 'label', 'caption'], dtype='object')\n",
      "image_path                                water_070_spatial.jpg\n",
      "label                                               Label.WATER\n",
      "caption       A kayaker wearing a blue wetsuit and black hel...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "metadata_path = Path(\"../data/augmented/metadata.csv\")\n",
    "df = pd.read_csv(metadata_path)\n",
    "print(df.columns)\n",
    "print(df.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e306381a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   taille de vocabulaire: 973\n",
      "   vocab_size: 974\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "vocab_size = 1000\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"caption\"])\n",
    "actual_vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"   taille de vocabulaire: {len(tokenizer.word_index)}\")\n",
    "print(f\"   vocab_size: {actual_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c58a2ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_path', 'label', 'caption'], dtype='object')\n",
      "image_path                                water_070_spatial.jpg\n",
      "label                                               Label.WATER\n",
      "caption       A kayaker wearing a blue wetsuit and black hel...\n",
      "Name: 1, dtype: object\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     17\u001b[39m df_train, df_temp = train_test_split(df, test_size=\u001b[32m0.3\u001b[39m, random_state=\u001b[32m11\u001b[39m)\n\u001b[32m     18\u001b[39m df_test, df_val = train_test_split(df_temp, test_size=\u001b[32m0.5\u001b[39m, random_state=\u001b[32m11\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m train_dataset = \u001b[43mCLIPDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmented_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_resnet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m val_dataset   = CLIPDataset(df_val, augmented_dir, tokenizer, transform_resnet)\n\u001b[32m     23\u001b[39m test_dataset  = CLIPDataset(df_test, augmented_dir, tokenizer, transform_resnet)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mCLIPDataset.__init__\u001b[39m\u001b[34m(self, df, base_dir, tokenizer, image_transform, max_length)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mself\u001b[39m.max_length = max_length\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# mapping constant\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mself\u001b[39m.class_to_idx = \u001b[43m{\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m.base_dir = base_dir\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mself\u001b[39m.max_length = max_length\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# mapping constant\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mself\u001b[39m.class_to_idx = {\u001b[38;5;28;43mcls\u001b[39;49m: i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m.labels))}\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m.base_dir = base_dir\n",
      "\u001b[31mNameError\u001b[39m: name 'cls' is not defined"
     ]
    }
   ],
   "source": [
    "metadata_path = Path(\"../data/augmented/metadata.csv\")\n",
    "df = pd.read_csv(metadata_path)\n",
    "print(df.columns)\n",
    "print(df.iloc[1])\n",
    "\n",
    "augmented_dir = Path(\"../data/augmented\")\n",
    "\n",
    "transform_resnet = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "df_train, df_temp = train_test_split(df, test_size=0.3, random_state=11)\n",
    "df_test, df_val = train_test_split(df_temp, test_size=0.5, random_state=11)\n",
    "\n",
    "\n",
    "train_dataset = CLIPDataset(df_train, augmented_dir, tokenizer, transform_resnet)\n",
    "val_dataset   = CLIPDataset(df_val, augmented_dir, tokenizer, transform_resnet)\n",
    "test_dataset  = CLIPDataset(df_test, augmented_dir, tokenizer, transform_resnet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53fb4e18",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SmallCLIP.__init__() missing 2 required positional arguments: 'vision_encoder' and 'text_encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m epochs = \u001b[32m11\u001b[39m\n\u001b[32m      2\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m clip_model = \u001b[43mSmallCLIP\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: SmallCLIP.__init__() missing 2 required positional arguments: 'vision_encoder' and 'text_encoder'"
     ]
    }
   ],
   "source": [
    "epochs = 11\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = SmallCLIP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6daa548",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m images, (input_ids, attention_mask), labels_text \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataloader\u001b[49m:\n\u001b[32m      6\u001b[39m         images = images.to(device)\n\u001b[32m      7\u001b[39m         input_ids = input_ids.to(device)\n",
      "\u001b[31mNameError\u001b[39m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(epochs):\n",
    "    for images, (input_ids, attention_mask), labels_text in dataloader:\n",
    "        images = images.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "        logits, img_emb, txt_emb = clip_model(images, input_ids)\n",
    "\n",
    "        loss = clip_loss(logits)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch\", epoch+1, \"loss\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e77c912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniclip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
