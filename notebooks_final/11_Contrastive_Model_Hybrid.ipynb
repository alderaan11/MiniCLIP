{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8200dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077ba054",
   "metadata": {},
   "source": [
    "### UTILS BASE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ce43bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Label(Enum):\n",
    "    DOG = \"dog\"\n",
    "    BIKE = \"bike\"\n",
    "    BALL = \"ball\"\n",
    "    WATER = \"water\"    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808cfff3",
   "metadata": {},
   "source": [
    "### UTILS METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62ef7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(filename: str):\n",
    "    return filename.split(\"_\")[0]\n",
    "\n",
    "\n",
    "def get_uuid(filename: str):\n",
    "    name = Path(filename).stem          \n",
    "    parts = name.split(\"_\")\n",
    "    return \"_\".join(parts[:2])          \n",
    "\n",
    "\n",
    "def build_augmented_path(img_path: Path, base_dir: Path):\n",
    "    img_path = Path(img_path)\n",
    "    filename = img_path.name\n",
    "    label = get_label(filename)\n",
    "    uuid = get_uuid(filename)\n",
    "    pres = \"_\".join(filename.split(\".\")[0].split(\"_\")[1:])\n",
    "    return base_dir / uuid / pres / filename\n",
    "\n",
    "\n",
    "def make_class_names(dataset):\n",
    "    # dataset.class_to_idx: {label_str: idx}\n",
    "    idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "    return [idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "\n",
    "\n",
    "def count_oov_pct(sequences, oov_id):\n",
    "    total = sum(1 for seq in sequences for tid in seq if tid != 0)\n",
    "    oov = sum(1 for seq in sequences for tid in seq if tid == oov_id)\n",
    "    return oov/total*100 if total > 0 else 0\n",
    "\n",
    "class_to_idx = {\n",
    "    \"ball\":  0,\n",
    "    \"bike\":  1,\n",
    "    \"dog\":   2,\n",
    "    \"water\": 3,\n",
    "}\n",
    "\n",
    "class_names = [\"ball\", \"bike\", \"dog\", \"water\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b288621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(model: nn.Module, ckpt_path: str, strict: bool = True):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    state = ckpt[\"state_dict\"] if isinstance(ckpt, dict) and \"state_dict\" in ckpt else ckpt\n",
    "    model.load_state_dict(state, strict=strict)\n",
    "    return model\n",
    "\n",
    "def load_without_classifier(model: nn.Module, ckpt_path: str, classifier_prefix=\"classifier.\", map_location=\"cpu\"):\n",
    "    ckpt = torch.load(ckpt_path, map_location=map_location)\n",
    "    state = ckpt[\"state_dict\"] if isinstance(ckpt, dict) and \"state_dict\" in ckpt else ckpt\n",
    "\n",
    "    filtered = {k: v for k, v in state.items() if not k.startswith(classifier_prefix)}\n",
    "    missing, unexpected = model.load_state_dict(filtered, strict=False)\n",
    "    print(\"Missing:\", missing)\n",
    "    print(\"Unexpected:\", unexpected)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba478c",
   "metadata": {},
   "source": [
    "### DATASET CLIP MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3034c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CLIPDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        base_dir: Path,\n",
    "        transform,\n",
    "        keras_tokenizer=None,\n",
    "        hf_tokenizer=None,\n",
    "        max_seq_len: int = 64,\n",
    "        vocab_size: int = 1000,\n",
    "        text_mode: str = \"keras\"  # 'keras', 'hf'\n",
    "    ):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.transform = transform\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = 1000\n",
    "        self.text_mode = text_mode\n",
    "        \n",
    "        self.img_paths = df[\"image_path\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.captions = df[\"caption\"].tolist()\n",
    "        \n",
    "        self.classes = sorted(set(self.labels))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "        self.num_classes = len(self.classes)\n",
    "        \n",
    "        self.keras_tokenizer = keras_tokenizer\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        \n",
    "        if self.keras_tokenizer is not None:\n",
    "            oov_tok = getattr(self.keras_tokenizer, \"oov_token\", \"<OOV>\")\n",
    "            self.oov_id = self.keras_tokenizer.word_index.get(oov_tok, 1)\n",
    "            if self.oov_id is None or self.oov_id >= self.vocab_size:\n",
    "                self.oov_id = 1\n",
    "        \n",
    "        self._validate_tokenizers()\n",
    "    \n",
    "    def _validate_tokenizers(self):\n",
    "        if self.text_mode in [\"keras\", \"both\"] and self.keras_tokenizer is None:\n",
    "            raise ValueError(\"keras_tokenizer requis pour text_mode='keras' ou 'both'\")\n",
    "        if self.text_mode in [\"hf\", \"both\"] and self.hf_tokenizer is None:\n",
    "            raise ValueError(\"hf_tokenizer requis pour text_mode='hf' ou 'both'\")\n",
    "    \n",
    "    def _build_image_path(self, img_path: str) -> Path:\n",
    "        return Path(build_augmented_path(img_path, self.base_dir))\n",
    "    \n",
    "    def _tokenize_keras(self, text: str) -> torch.Tensor:\n",
    "        seq = self.keras_tokenizer.texts_to_sequences([text])[0]\n",
    "        seq = [\n",
    "            t if (t is not None and 0 <= t < self.vocab_size) else self.oov_id\n",
    "            for t in seq\n",
    "        ]\n",
    "        padded = pad_sequences(\n",
    "            [seq],\n",
    "            maxlen=self.max_seq_len,\n",
    "            padding=\"post\",\n",
    "            truncating=\"post\",\n",
    "            value=0\n",
    "        )[0].astype(np.int64)\n",
    "        \n",
    "        return torch.tensor(padded, dtype=torch.long)\n",
    "    \n",
    "    def _tokenize_hf(self, text: str) -> dict:\n",
    "        enc = self.hf_tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_seq_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0).to(torch.long),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0).to(torch.long)\n",
    "        }\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        img_path = self._build_image_path(self.img_paths[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        caption = str(self.captions[idx])\n",
    "        \n",
    "        label_str = self.labels[idx]\n",
    "        label = torch.tensor(self.class_to_idx[label_str], dtype=torch.long)\n",
    "        \n",
    "        output = {\n",
    "            \"idx\": torch.tensor(idx, dtype=torch.long),\n",
    "            \"image\": img,\n",
    "            \"label\": label,\n",
    "            \"caption\": caption,\n",
    "        }\n",
    "        \n",
    "        if self.text_mode == \"keras\":\n",
    "            input_ids_keras = self._tokenize_keras(caption)\n",
    "            output[\"input_ids_keras\"] = input_ids_keras\n",
    "            output[\"attention_mask_keras\"] = (input_ids_keras != 0).long()\n",
    "            \n",
    "            if self.text_mode == \"keras\":\n",
    "                output[\"input_ids\"] = input_ids_keras\n",
    "                output[\"attention_mask\"] = output[\"attention_mask_keras\"]\n",
    "        \n",
    "        if self.text_mode == \"hf\":\n",
    "            hf_tokens = self._tokenize_hf(caption)\n",
    "            output[\"input_ids_hf\"] = hf_tokens[\"input_ids\"]\n",
    "            output[\"attention_mask_hf\"] = hf_tokens[\"attention_mask\"]\n",
    "            \n",
    "            if self.text_mode == \"hf\":\n",
    "                output[\"input_ids\"] = hf_tokens[\"input_ids\"]\n",
    "                output[\"attention_mask\"] = hf_tokens[\"attention_mask\"]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    \n",
    "    def _get_img_path_from_idx(self, idx: int) -> Path:\n",
    "        return self._build_image_path(self.img_paths[idx])\n",
    "    \n",
    "    def _get_caption_from_idx(self, idx: int) -> str:\n",
    "        return self.captions[idx]\n",
    "    \n",
    "    def _get_label_from_idx(self, idx: int) -> str:\n",
    "        return self.labels[idx]\n",
    "    \n",
    "    def _get_img_size(self, idx: int) -> Tuple[int, int]:\n",
    "        img_path = self._build_image_path(self.img_paths[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            for t in self.transform.transforms:\n",
    "                if isinstance(t, transforms.Resize):\n",
    "                    img = t(img)\n",
    "        return img.height, img.width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd07af4c",
   "metadata": {},
   "source": [
    "### TRANSFORMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4aff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((300, 500)),\n",
    "        transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7825b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_resnet = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b609814",
   "metadata": {},
   "source": [
    "### VISION CNN ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dedee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBasic(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2,2)\n",
    "        )\n",
    "\n",
    "        self.flattened_size = self._get_flattened_size()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def _get_flattened_size(self):\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn(1, 3, 300, 500)\n",
    "            x = self.features(x)\n",
    "            return x.view(1, -1).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398dc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoderFromCNNBasic(nn.Module):\n",
    "    def __init__(self, cnn_basic: nn.Module, embed_dim: int = 256, proj_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.features = cnn_basic.features\n",
    "\n",
    "        self.flattened_size = cnn_basic.flattened_size\n",
    "        self.backbone_fc = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(embed_dim, proj_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.backbone_fc(x)\n",
    "        x = self.proj(x)\n",
    "        x = F.normalize(x, dim=-1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccd7cfb",
   "metadata": {},
   "source": [
    "### VISUAL RESNET ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51ea34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "class ImageEncoderFromResNet18(nn.Module):\n",
    "    def __init__(self, resnet: nn.Module, proj_dim: int = 256, train_backbone: bool = True):\n",
    "        super().__init__()\n",
    "        self.backbone = resnet\n",
    "\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.proj = nn.Linear(512, proj_dim)\n",
    "\n",
    "        if not train_backbone:\n",
    "            for p in self.backbone.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        feats = self.backbone(x)\n",
    "        z = self.proj(feats)\n",
    "        return F.normalize(z, dim=-1)\n",
    "def load_best_resnet18(num_classes=4, ckpt_path=\"../notebooks/best-model-resnet.pth\", device=\"cpu\"):\n",
    "    model = resnet18(weights=None)  # weights=None car on charge tes poids\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    state = ckpt[\"model_state_dict\"] if isinstance(ckpt, dict) and \"model_state_dict\" in ckpt else ckpt\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b00fa",
   "metadata": {},
   "source": [
    "### SMALL BERT TEXT ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10fa38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, sequence_length: int, vocab_size:int, embed_dim:int):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(sequence_length, embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len = x.size()\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        return self.token_embeddings(x) + self.position_embeddings(positions)\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout_rate: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout_rate, batch_first=True)\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, padding_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        attn_output, _ = self.att(\n",
    "            x, x, x,\n",
    "            key_padding_mask=padding_mask  # <-- correct masking\n",
    "        )\n",
    "\n",
    "        x = self.layernorm1(x + self.dropout1(attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        out = self.layernorm2(x + self.dropout2(ffn_output))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ee20bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallBERT(nn.Module):\n",
    "    def __init__(self, sequence_length: int, vocab_size: int, embed_dim: int,\n",
    "                 num_heads: int, ff_dim: int, num_layers: int) -> None:\n",
    "        super().__init__()\n",
    "        self.pos_embedding = PositionalEmbedding(sequence_length, vocab_size, embed_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len) token ids\n",
    "        \"\"\"\n",
    "        padding_mask = (x == 0)  # 0 = PAD  -> True masked in attn\n",
    "        x = self.pos_embedding(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, padding_mask=padding_mask)\n",
    "\n",
    "        x = self.layernorm(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class SmallBERTPourClassification(nn.Module):\n",
    "    def __init__(self, sequence_length: int, vocab_size: int, embed_dim: int,\n",
    "                 num_heads: int, ff_dim: int, num_layers: int,\n",
    "                 num_classes: int = 4) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = SmallBERT(sequence_length, vocab_size, embed_dim, num_heads, ff_dim, num_layers)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        enc = self.encoder(x)             # (batch, seq_len, embed_dim)\n",
    "        pooled = enc.mean(dim=1)          # mean pooling\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits  # logits only, pas softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac6cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoderFromSmallBERT(nn.Module):\n",
    "    def __init__(self, text_model: nn.Module, proj_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.encoder = text_model.encoder \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        embed_dim = text_model.classifier.in_features\n",
    "\n",
    "        self.proj = nn.Linear(embed_dim, proj_dim)\n",
    "\n",
    "    def forward(self, x, padding_mask=None):\n",
    "        enc = self.encoder(x)  \n",
    "        pooled = enc.mean(dim=1)\n",
    "        pooled = self.dropout(pooled)\n",
    "        z = self.proj(pooled)\n",
    "        z = F.normalize(z, dim=-1)\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225bc01d",
   "metadata": {},
   "source": [
    "### TEXT DISTILLBERT ENCODER MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aecc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoderFromDistilBERT(nn.Module):\n",
    "    def __init__(self, distilbert_cls_model, proj_dim=256):\n",
    "        super().__init__()\n",
    "        self.backbone = distilbert_cls_model.distilbert\n",
    "        hidden = self.backbone.config.hidden_size\n",
    "        self.proj = nn.Linear(hidden, proj_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        h = out.last_hidden_state  \n",
    "\n",
    "        mask = attention_mask.unsqueeze(-1).float()         \n",
    "        summed = (h * mask).sum(dim=1)                      \n",
    "        denom = mask.sum(dim=1).clamp(min=1e-6)              \n",
    "        pooled = summed / denom                             \n",
    "\n",
    "        pooled = self.dropout(pooled)\n",
    "        z = self.proj(pooled)\n",
    "        return F.normalize(z, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hf_classifier_checkpoint(model, ckpt_path, map_location=\"cpu\"):\n",
    "    ckpt = torch.load(ckpt_path, map_location=map_location)\n",
    "\n",
    "    if isinstance(ckpt, dict) and any(k.startswith(\"distilbert.\") or k.startswith(\"classifier.\") for k in ckpt.keys()):\n",
    "        state = ckpt\n",
    "    elif isinstance(ckpt, dict) and \"state_dict\" in ckpt:\n",
    "        state = ckpt[\"state_dict\"]\n",
    "    elif isinstance(ckpt, dict) and \"model_state_dict\" in ckpt:\n",
    "        state = ckpt[\"model_state_dict\"]\n",
    "    else:\n",
    "        raise ValueError(\"Format checkpoint non reconnu\")\n",
    "\n",
    "    missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "    print(\"Missing:\", missing)\n",
    "    print(\"Unexpected:\", unexpected)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ffd23",
   "metadata": {},
   "source": [
    "### CONTRASTIVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ff0118",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPLikeModel(nn.Module):\n",
    "    def __init__(self, image_encoder: nn.Module, text_encoder: nn.Module, init_logit_scale=1/0.07):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(float(torch.log(torch.tensor(init_logit_scale)))))\n",
    "\n",
    "    def forward(self, images, tokens, padding_mask=None):\n",
    "        img = self.image_encoder(images)                   #(B, D)\n",
    "        txt = self.text_encoder(tokens, padding_mask)      #(B, D)\n",
    "        logit_scale = self.logit_scale.exp().clamp(1e-3, 100.0)\n",
    "        logits = logit_scale * (img @ txt.t())             #(B, B)\n",
    "        return logits, img, txt\n",
    "\n",
    "\n",
    "def clip_contrastive_loss(logits):\n",
    "    b = logits.size(0)\n",
    "    labels = torch.arange(b, device=logits.device)\n",
    "\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.t(), labels)\n",
    "    return (loss_i2t + loss_t2i) / 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd79aa0a",
   "metadata": {},
   "source": [
    "### CONTRASTIVE HF MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae1add9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPLikeModelHF(nn.Module):\n",
    "    def __init__(self, image_encoder, text_encoder, init_temp=0.07):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1.0/init_temp)))\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        img = self.image_encoder(images)                         \n",
    "        txt = self.text_encoder(input_ids, attention_mask)       \n",
    "        scale = self.logit_scale.exp().clamp(1e-3, 100.0)\n",
    "        logits = scale * (img @ txt.t())\n",
    "        return logits, img, txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed935e7b",
   "metadata": {},
   "source": [
    "### TRAINING METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.should_stop = False\n",
    "\n",
    "    def step(self, metric):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = metric\n",
    "        elif metric < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "        else:\n",
    "            self.best_score = metric\n",
    "            self.counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fff583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_collate_keras(batch):\n",
    "    images = torch.stack([b[\"image\"] for b in batch], dim=0)  # (B,3,H,W)\n",
    "    input_ids = torch.stack([b[\"input_ids\"] for b in batch], dim=0)  # (B,L)\n",
    "    attention_mask = torch.stack([b[\"attention_mask\"] for b in batch], dim=0)  # (B,L)\n",
    "    labels = torch.stack([b[\"label\"] for b in batch], dim=0)  # (B,)\n",
    "    idx = torch.stack([b[\"idx\"] for b in batch], dim=0)  # (B,)\n",
    "    return {\"images\": images, \"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels, \"idx\": idx}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc3b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch, model, optimizer, device=\"cuda\"):\n",
    "    images, tokens = batch  # + éventuellement padding_mask\n",
    "    images = images.to(device)\n",
    "    tokens = tokens.to(device)\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits, _, _ = model(images, tokens)\n",
    "    loss = clip_contrastive_loss(logits)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "@torch.no_grad()\n",
    "def val_step(batch, model, device=\"cuda\"):\n",
    "    images = batch[\"images\"].to(device)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    padding_mask = (attention_mask == 0)\n",
    "\n",
    "    model.eval()\n",
    "    logits, _, _ = model(images, input_ids, padding_mask=padding_mask)\n",
    "    loss = clip_contrastive_loss(logits)\n",
    "    return loss.item()\n",
    "def fit(model, train_loader, val_loader, optimizer, epochs=10, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_losses = []\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch} [train]\"):\n",
    "            train_losses.append(train_step(batch, model, optimizer, device))\n",
    "\n",
    "        val_losses = []\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch} [val]\"):\n",
    "            val_losses.append(val_step(batch, model, device))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch} | \"\n",
    "            f\"train_loss={sum(train_losses)/len(train_losses):.4f} | \"\n",
    "            f\"val_loss={sum(val_losses)/len(val_losses):.4f}\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_hf(batch, model, optimizer, device=\"cuda\"):\n",
    "    images = batch[\"images\"].to(device)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    logits, _, _ = model(images, input_ids, attention_mask=attention_mask)\n",
    "    loss = clip_contrastive_loss(logits)\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_step_hf(batch, model, device=\"cuda\"):\n",
    "    images = batch[\"images\"].to(device)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "    model.eval()\n",
    "    logits, _, _ = model(images, input_ids, attention_mask=attention_mask)\n",
    "    loss = clip_contrastive_loss(logits)\n",
    "    return loss.item()\n",
    "\n",
    "def fit_hf(model, train_loader, val_loader, optimizer, epochs=10, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            train_losses.append(train_step_hf(batch, model, optimizer, device))\n",
    "\n",
    "        val_losses = []\n",
    "        for batch in val_loader:\n",
    "            val_losses.append(val_step_hf(batch, model, device))\n",
    "\n",
    "        print(f\"Epoch {epoch} | train_loss={sum(train_losses)/len(train_losses):.4f} | val_loss={sum(val_losses)/len(val_losses):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06483c",
   "metadata": {},
   "source": [
    "### EVAL METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcfdf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def recall_at_k_from_sims(sims: torch.Tensor, ks=(1,5,10)):\n",
    "    N = sims.size(0)\n",
    "    gt = torch.arange(N, device=sims.device).unsqueeze(1)  # (N,1)\n",
    "    ranks = torch.argsort(sims, dim=1, descending=True)     # (N,N)\n",
    "    out = {}\n",
    "    for k in ks:\n",
    "        hit = (ranks[:, :k] == gt).any(dim=1).float().mean().item()\n",
    "        out[f\"R@{k}\"] = hit\n",
    "    return out\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_clip_recall(index, ks=(1,5,10), device=\"cpu\"):\n",
    "    img = index[\"img_embs\"].to(device)\n",
    "    txt = index[\"txt_embs\"].to(device)\n",
    "\n",
    "    sims_t2i = txt @ img.t()\n",
    "    sims_i2t = img @ txt.t()\n",
    "\n",
    "    t2i = recall_at_k_from_sims(sims_t2i, ks=ks)\n",
    "    i2t = recall_at_k_from_sims(sims_i2t, ks=ks)\n",
    "    return t2i, i2t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4972fa",
   "metadata": {},
   "source": [
    "### RETRIEVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08253e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_keras(tokenizer, text: str, max_seq_len=64, vocab_size=1000, oov_id=1):\n",
    "    seq = tokenizer.texts_to_sequences([text])[0]\n",
    "    seq = [t if (t is not None and 0 <= t < vocab_size) else oov_id for t in seq]\n",
    "\n",
    "    padded = pad_sequences(\n",
    "        [seq], maxlen=max_seq_len, padding=\"post\", truncating=\"post\", value=0\n",
    "    )[0].astype(np.int64)\n",
    "\n",
    "    input_ids = torch.tensor(padded, dtype=torch.long).unsqueeze(0)      \n",
    "    attention_mask = (input_ids != 0).long()                             \n",
    "    padding_mask = (attention_mask == 0)                                 \n",
    "    return input_ids, padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e08f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_collate(batch):\n",
    "    images = torch.stack([b[\"image\"] for b in batch], dim=0) \n",
    "    input_ids = torch.stack([b[\"input_ids\"] for b in batch], dim=0)  \n",
    "    attention_mask = torch.stack([b[\"attention_mask\"] for b in batch], dim=0) \n",
    "\n",
    "    labels = torch.stack([b[\"label\"] for b in batch], dim=0) if \"label\" in batch[0] else None\n",
    "    idx = torch.stack([b[\"idx\"] for b in batch], dim=0) if \"idx\" in batch[0] else None\n",
    "\n",
    "    out = {\n",
    "        \"images\": images,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "    if labels is not None:\n",
    "        out[\"labels\"] = labels\n",
    "    if idx is not None:\n",
    "        out[\"idx\"] = idx\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c272b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def show_retrieval_from_text(\n",
    "    clip_model,\n",
    "    index,\n",
    "    query_text: str,\n",
    "    tokenizer,\n",
    "    max_seq_len=64,\n",
    "    vocab_size=1000,\n",
    "    oov_id=1,\n",
    "    topk=5,\n",
    "    device=\"cuda\",\n",
    "    show_images=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Donne une phrase -> retourne topk images (paths + scores) et optionnellement les affiche.\n",
    "    \"\"\"\n",
    "    clip_model.eval()\n",
    "    clip_model.to(device)\n",
    "\n",
    "    input_ids, padding_mask = encode_text_keras(\n",
    "        tokenizer, query_text, max_seq_len=max_seq_len,\n",
    "        vocab_size=vocab_size, oov_id=oov_id\n",
    "    )\n",
    "    input_ids = input_ids.to(device)\n",
    "    padding_mask = padding_mask.to(device)\n",
    "    q = clip_model.text_encoder(input_ids, padding_mask)   \n",
    "    q = F.normalize(q, dim=-1).cpu()                       \n",
    "\n",
    "    sims = (index[\"img_embs\"] @ q.squeeze(0))            \n",
    "    topv, topi = torch.topk(sims, k=min(topk, sims.size(0)))\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(topv.tolist(), topi.tolist()):\n",
    "        path = index[\"image_paths\"][idx] if index[\"image_paths\"] is not None else None\n",
    "        cap = index[\"captions\"][idx] if index[\"captions\"] is not None else None\n",
    "        results.append({\"rank\": len(results)+1, \"score\": float(score), \"image_path\": path, \"caption\": cap})\n",
    "\n",
    "    print(f\"Query text: {query_text}\\nTop-{len(results)} images:\")\n",
    "    for r in results:\n",
    "        print(f\"#{r['rank']} score={r['score']:.4f} | {r['image_path']}\")\n",
    "\n",
    "    if show_images and index[\"image_paths\"] is not None:\n",
    "        fig = plt.figure(figsize=(14, 6))\n",
    "        for j, r in enumerate(results):\n",
    "            ax = fig.add_subplot(1, len(results), j+1)\n",
    "            img = Image.open(r[\"image_path\"]).convert(\"RGB\")\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(f\"#{r['rank']}\\n{r['score']:.3f}\")\n",
    "            ax.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759bf11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def show_retrieval_from_image(\n",
    "    clip_model,\n",
    "    index,\n",
    "    query_image,            # str path OR PIL.Image OR torch.Tensor \n",
    "    transform=None,         # même transform que ton dataset\n",
    "    topk=5,\n",
    "    device=\"cuda\",\n",
    "    show=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Donne une image -> retourne topk captions (et leurs images associées si disponibles).\n",
    "    \"\"\"\n",
    "    clip_model.eval()\n",
    "    clip_model.to(device)\n",
    "\n",
    "    # Prépare tensor image (1,3,H,W)\n",
    "    if isinstance(query_image, str):\n",
    "        img = Image.open(query_image).convert(\"RGB\")\n",
    "        x = transform(img) if transform is not None else img\n",
    "    elif isinstance(query_image, Image.Image):\n",
    "        x = transform(query_image) if transform is not None else query_image\n",
    "    elif torch.is_tensor(query_image):\n",
    "        x = query_image\n",
    "    else:\n",
    "        raise TypeError(\"query_image doit être un chemin, une PIL.Image, ou un torch.Tensor.\")\n",
    "\n",
    "    if not torch.is_tensor(x):\n",
    "        raise ValueError(\"transform doit retourner un torch.Tensor (3,H,W).\")\n",
    "\n",
    "    x = x.unsqueeze(0).to(device)  #(1,3,H,W)\n",
    "\n",
    "    q = clip_model.image_encoder(x)          #(1,D)\n",
    "    q = F.normalize(q, dim=-1).cpu()         #(1,D)\n",
    "\n",
    "    sims = (index[\"txt_embs\"] @ q.squeeze(0))   #(N,)\n",
    "    topv, topi = torch.topk(sims, k=min(topk, sims.size(0)))\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(topv.tolist(), topi.tolist()):\n",
    "        cap = index[\"captions\"][idx] if index[\"captions\"] is not None else None\n",
    "        path = index[\"image_paths\"][idx] if index[\"image_paths\"] is not None else None\n",
    "        results.append({\"rank\": len(results)+1, \"score\": float(score), \"caption\": cap, \"image_path\": path})\n",
    "\n",
    "    print(\"Top captions:\")\n",
    "    for r in results:\n",
    "        print(f\"#{r['rank']} score={r['score']:.4f} | {r['caption']}\")\n",
    "\n",
    "    if show:\n",
    "        fig = plt.figure(figsize=(14, 6))\n",
    "        ax0 = fig.add_subplot(1, len(results)+1, 1)\n",
    "        if isinstance(query_image, str):\n",
    "            qimg = Image.open(query_image).convert(\"RGB\")\n",
    "        elif isinstance(query_image, Image.Image):\n",
    "            qimg = query_image\n",
    "        else:\n",
    "            qimg = None\n",
    "        if qimg is not None:\n",
    "            ax0.imshow(qimg)\n",
    "        ax0.set_title(\"QUERY\")\n",
    "        ax0.axis(\"off\")\n",
    "\n",
    "        if index[\"image_paths\"] is not None:\n",
    "            for j, r in enumerate(results):\n",
    "                ax = fig.add_subplot(1, len(results)+1, j+2)\n",
    "                img = Image.open(r[\"image_path\"]).convert(\"RGB\")\n",
    "                ax.imshow(img)\n",
    "                title = f\"#{r['rank']} {r['score']:.3f}\"\n",
    "                ax.set_title(title)\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def build_clip_index(clip_model, dataloader, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Calcule embeddings normalisés pour tout le dataset du dataloader.\n",
    "    \"\"\"\n",
    "    clip_model.eval()\n",
    "    clip_model.to(device)\n",
    "\n",
    "    img_embs = []\n",
    "    txt_embs = []\n",
    "    captions = []\n",
    "    labels = []\n",
    "    idxs = []\n",
    "\n",
    "    ds = dataloader.dataset\n",
    "    has_utils = hasattr(ds, \"_get_img_path_from_idx\") and hasattr(ds, \"_get_caption_from_idx\")\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Building CLIP index\"):\n",
    "        images = batch[\"images\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        padding_mask = (attention_mask == 0)\n",
    "\n",
    "        im = clip_model.image_encoder(images)                         \n",
    "        tx = clip_model.text_encoder(input_ids, padding_mask)        \n",
    "\n",
    "    \n",
    "        im = F.normalize(im, dim=-1)\n",
    "        tx = F.normalize(tx, dim=-1)\n",
    "\n",
    "        img_embs.append(im.cpu())\n",
    "        txt_embs.append(tx.cpu())\n",
    "\n",
    "        if \"idx\" in batch:\n",
    "            idx_batch = batch[\"idx\"].cpu().tolist()\n",
    "            idxs.extend(idx_batch)\n",
    "\n",
    "            if has_utils:\n",
    "                for i in idx_batch:\n",
    "                    captions.append(str(ds._get_caption_from_idx(i)))\n",
    "        if \"labels\" in batch:\n",
    "            labels.extend(batch[\"labels\"].cpu().tolist())\n",
    "\n",
    "    img_embs = torch.cat(img_embs, dim=0)  \n",
    "    txt_embs = torch.cat(txt_embs, dim=0)  \n",
    "\n",
    "    image_paths = None\n",
    "    if has_utils and len(idxs) == img_embs.size(0):\n",
    "        image_paths = [str(ds._get_img_path_from_idx(i)) for i in idxs]\n",
    "\n",
    "    return {\n",
    "        \"img_embs\": img_embs,\n",
    "        \"txt_embs\": txt_embs,\n",
    "        \"image_paths\": image_paths,\n",
    "        \"captions\": captions if len(captions) == img_embs.size(0) else None,\n",
    "        \"labels\": labels if len(labels) == img_embs.size(0) else None,\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_clip_index_hf(clip_model, dataloader, device=\"cuda\"):\n",
    "    clip_model.eval()\n",
    "    clip_model.to(device)\n",
    "\n",
    "    img_embs = []\n",
    "    txt_embs = []\n",
    "    captions = []\n",
    "    image_paths = []\n",
    "\n",
    "    ds = dataloader.dataset\n",
    "    has_utils = hasattr(ds, \"_get_img_path_from_idx\") and hasattr(ds, \"_get_caption_from_idx\")\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Building CLIP index (HF)\"):\n",
    "        images = batch[\"images\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        img = clip_model.image_encoder(images)                      \n",
    "        txt = clip_model.text_encoder(input_ids, attention_mask)   \n",
    "\n",
    "        img = F.normalize(img, dim=-1)\n",
    "        txt = F.normalize(txt, dim=-1)\n",
    "\n",
    "        img_embs.append(img.cpu())\n",
    "        txt_embs.append(txt.cpu())\n",
    "\n",
    "        if \"idx\" in batch and has_utils:\n",
    "            for i in batch[\"idx\"].tolist():\n",
    "                image_paths.append(str(ds._get_img_path_from_idx(i)))\n",
    "                captions.append(str(ds._get_caption_from_idx(i)))\n",
    "\n",
    "    return {\n",
    "        \"img_embs\": torch.cat(img_embs, dim=0),   \n",
    "        \"txt_embs\": torch.cat(txt_embs, dim=0),   \n",
    "        \"image_paths\": image_paths,\n",
    "        \"captions\": captions,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b7854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def show_retrieval_from_text_hf(\n",
    "    clip_model,\n",
    "    index,\n",
    "    query_text: str,\n",
    "    tokenizer,\n",
    "    max_len=128,\n",
    "    topk=5,\n",
    "    device=\"cuda\",\n",
    "    show_images=True\n",
    "):\n",
    "    clip_model.eval()\n",
    "    clip_model.to(device)\n",
    "\n",
    "    enc = tokenizer(\n",
    "        query_text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_len,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device)\n",
    "\n",
    "    q = clip_model.text_encoder(input_ids, attention_mask)   \n",
    "    q = F.normalize(q, dim=-1).cpu()\n",
    "\n",
    "    sims = index[\"img_embs\"] @ q.squeeze(0)             \n",
    "    topv, topi = torch.topk(sims, k=min(topk, sims.size(0)))\n",
    "\n",
    "    print(f\"Query text: {query_text}\\nTop-{topk} images:\")\n",
    "\n",
    "    results = []\n",
    "    for r, (score, idx) in enumerate(zip(topv.tolist(), topi.tolist()), 1):\n",
    "        path = index[\"image_paths\"][idx]\n",
    "        print(f\"#{r} score={score:.4f} | {path}\")\n",
    "        results.append((score, path))\n",
    "\n",
    "    if show_images:\n",
    "        fig = plt.figure(figsize=(14, 5))\n",
    "        for j, (score, path) in enumerate(results):\n",
    "            ax = fig.add_subplot(1, len(results), j+1)\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(f\"{score:.3f}\")\n",
    "            ax.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9888058",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def show_retrieval_from_image_hf(\n",
    "    clip_model,\n",
    "    index,\n",
    "    query_image,              \n",
    "    transform,\n",
    "    topk=5,\n",
    "    device=\"cuda\",\n",
    "    show=True\n",
    "):\n",
    "    clip_model.eval()\n",
    "    clip_model.to(device)\n",
    "\n",
    "    if isinstance(query_image, str):\n",
    "        img = Image.open(query_image).convert(\"RGB\")\n",
    "        x = transform(img)\n",
    "        query_vis = img\n",
    "    elif isinstance(query_image, Image.Image):\n",
    "        x = transform(query_image)\n",
    "        query_vis = query_image\n",
    "    elif torch.is_tensor(query_image):\n",
    "        x = query_image\n",
    "        query_vis = None\n",
    "    else:\n",
    "        raise TypeError(\"query_image doit être str, PIL.Image ou Tensor\")\n",
    "\n",
    "    x = x.unsqueeze(0).to(device)\n",
    "\n",
    "    q = clip_model.image_encoder(x)        \n",
    "    q = F.normalize(q, dim=-1).cpu()\n",
    "\n",
    "    sims = index[\"txt_embs\"] @ q.squeeze(0)  \n",
    "    topv, topi = torch.topk(sims, k=min(topk, sims.size(0)))\n",
    "\n",
    "    print(\"Top captions:\")\n",
    "    results = []\n",
    "    for r, (score, idx) in enumerate(zip(topv.tolist(), topi.tolist()), 1):\n",
    "        cap = index[\"captions\"][idx]\n",
    "        print(f\"#{r} score={score:.4f} | {cap}\")\n",
    "        results.append((score, cap, index[\"image_paths\"][idx]))\n",
    "\n",
    "    if show:\n",
    "        fig = plt.figure(figsize=(14, 5))\n",
    "        ax0 = fig.add_subplot(1, topk+1, 1)\n",
    "        ax0.imshow(query_vis)\n",
    "        ax0.set_title(\"QUERY\")\n",
    "        ax0.axis(\"off\")\n",
    "\n",
    "        for j, (score, _, path) in enumerate(results):\n",
    "            ax = fig.add_subplot(1, topk+1, j+2)\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(f\"{score:.3f}\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f6bc71",
   "metadata": {},
   "source": [
    "### DATASET LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b636a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = Path(\"../data/final_dataset_noaug2/metadata.csv\")\n",
    "base_dir = Path(\"../data/final_dataset_noaug2\")\n",
    "\n",
    "df = pd.read_csv(metadata_path)\n",
    "print(df.columns)\n",
    "print(df.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51e3c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train, df_temp = train_test_split(df, test_size=0.3, random_state=11, stratify=df[\"label\"])\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=11, stratify=df_temp[\"label\"])\n",
    "\n",
    "\n",
    "print(df_train[\"label\"].value_counts(normalize=True) * 100)\n",
    "print(df_val[\"label\"].value_counts(normalize=True) * 100)\n",
    "print(df_test[\"label\"].value_counts(normalize=True) * 100)\n",
    "\n",
    "X_train, y_train, caption_train = df_train[\"image_path\"], df_train[\"label\"], df_train[\"caption\"]\n",
    "X_val, y_val, caption_val  = df_val[\"image_path\"], df_val[\"label\"], df_val[\"caption\"]\n",
    "X_test, y_test, caption_test   = df_test[\"image_path\"], df_test[\"label\"], df_test[\"caption\"]\n",
    "\n",
    "X_train = caption_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_val = caption_val.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "X_test = caption_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "print(\"Avant :\", len(df))\n",
    "print(\"Après  :\", len(df))\n",
    "print(\"Doublons supprimés :\", len(df) - len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0352d1",
   "metadata": {},
   "source": [
    "### TRAINING CNN SMALLBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79256b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_basic = CNNBasic(num_classe=4)\n",
    "\n",
    "text_cls = SmallBERTPourClassification(\n",
    "    vocab_size=1000,      #atcher les ids de tokens\n",
    "    sequence_length=128,   #matcher la longueur des séquences du modèle chargé\n",
    "    embed_dim=128,\n",
    "    num_heads=4,\n",
    "    ff_dim=256,\n",
    "    num_layers=2,\n",
    "    num_classes=4\n",
    ")\n",
    "\n",
    "cnn_basic = load_model_weights(cnn_basic, \"../models/best-model-ccnbasic.pth\", strict=True)\n",
    "text_cls  = load_model_weights(text_cls,  \"../models/best-model-smallbert.pth\", strict=True)\n",
    "\n",
    "img_encoder = ImageEncoderFromCNNBasic(cnn_basic, embed_dim=256, proj_dim=256)\n",
    "txt_encoder = TextEncoderFromSmallBERT(text_cls, proj_dim=256)\n",
    "\n",
    "clip_model = CLIPLikeModel(img_encoder, txt_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ead7f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OVV>\")\n",
    "tokenizer.fit_on_texts(df[\"caption\"])\n",
    "\n",
    "train_ds = CLIPDataset(df_train, base_dir=base_dir, transform=transform,\n",
    "                       keras_tokenizer=tokenizer, max_seq_len=64,\n",
    "                       vocab_size=1000, text_mode=\"keras\")\n",
    "\n",
    "val_ds = CLIPDataset(df_val, base_dir=base_dir, transform=transform,\n",
    "                     keras_tokenizer=tokenizer, max_seq_len=64,\n",
    "                     vocab_size=1000, text_mode=\"keras\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=4,\n",
    "                          pin_memory=True, collate_fn=clip_collate_keras)\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=4,\n",
    "                        pin_memory=True, collate_fn=clip_collate_keras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61859a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_encoder = ImageEncoderFromCNNBasic(cnn_basic, embed_dim=256, proj_dim=256)\n",
    "txt_encoder = TextEncoderFromSmallBERT(text_cls, proj_dim=256)\n",
    "clip_model = CLIPLikeModel(img_encoder, txt_encoder)\n",
    "\n",
    "def set_trainable(m, flag: bool):\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "set_trainable(clip_model.image_encoder.features, False)\n",
    "set_trainable(clip_model.text_encoder.encoder, False)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, clip_model.parameters()),\n",
    "    lr=1e-3, weight_decay=1e-4\n",
    ")\n",
    "\n",
    "fit(clip_model, train_loader, val_loader, optimizer, epochs=3, device=device)\n",
    "index_val = build_clip_index(clip_model, val_loader, device=device)\n",
    "res = show_retrieval_from_text(\n",
    "    clip_model, index_val,\n",
    "    query_text=\"the man playing with a ball\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=64, vocab_size=1000, oov_id=1,\n",
    "    topk=5, device=device, show_images=True\n",
    ")\n",
    "\n",
    "res = show_retrieval_from_image(\n",
    "    clip_model, index_val,\n",
    "    query_image=str(val_ds._get_img_path_from_idx(121\n",
    ")), \n",
    "    transform=transform,\n",
    "    topk=5, device=device, show=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39253f5",
   "metadata": {},
   "source": [
    "### CLIP DISTILLBERT CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7711f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"  \n",
    "distilbert_cls = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=4)\n",
    "distilbert_cls = load_hf_classifier_checkpoint(distilbert_cls, \"../models/best-distilbert.pth\")\n",
    "\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_ds = CLIPDataset(df_train, base_dir=base_dir, transform=transform,\n",
    "                       hf_tokenizer=hf_tokenizer, max_seq_len=128,\n",
    "                       text_mode=\"hf\")\n",
    "\n",
    "val_ds = CLIPDataset(df_val, base_dir=base_dir, transform=transform,\n",
    "                     hf_tokenizer=hf_tokenizer, max_seq_len=128,\n",
    "                     text_mode=\"hf\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True,\n",
    "                          collate_fn=clip_collate)\n",
    "\n",
    "val_loader = DataLoader(val_ds, batch_size=64, shuffle=False,\n",
    "                        num_workers=4, pin_memory=True,\n",
    "                        collate_fn=clip_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a0d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image encoder (comme avant)\n",
    "img_encoder = ImageEncoderFromCNNBasic(cnn_basic, embed_dim=256, proj_dim=256)\n",
    "\n",
    "# texte encoder distilbert\n",
    "# txt_encoder = TextEncoderFromDistilBERT(distilbert_cls, proj_dim=256)\n",
    "txt_encoder = TextEncoderFromSmallBERT(text_cls, proj_dim=256)\n",
    "\n",
    "clip_model = CLIPLikeModelHF(img_encoder, txt_encoder, init_temp=0.07)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model.to(device)\n",
    "def set_trainable(m, flag: bool):\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "# Freeze CNN features + distilbert backbone au début\n",
    "set_trainable(clip_model.image_encoder.features, False)\n",
    "set_trainable(clip_model.text_encoder.backbone, False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, clip_model.parameters()),\n",
    "                              lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "# fit version HF (en appelant train_step_hf/val_step_hf)\n",
    "set_trainable(clip_model.image_encoder.features, True)\n",
    "set_trainable(clip_model.text_encoder.backbone, True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(clip_model.parameters(), lr=1e-4, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a2e74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_val = build_clip_index_hf(clip_model, val_loader, device=device)\n",
    "\n",
    "show_retrieval_from_text_hf(\n",
    "    clip_model, index_val,\n",
    "    query_text=\"a man riding a bike\",\n",
    "    tokenizer=hf_tokenizer,\n",
    "    max_len=128,\n",
    "    topk=5,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "show_retrieval_from_image_hf(\n",
    "    clip_model, index_val,\n",
    "    query_image=Image.open(val_ds._get_img_path_from_idx(121)),\n",
    "    transform=transform,\n",
    "    topk=5,\n",
    "    device=device\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb466dd",
   "metadata": {},
   "source": [
    "### TRAINING SMALLBERT RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6cdd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_resnet = CLIPDataset(df_train, base_dir=base_dir, transform=transform_resnet,\n",
    "                       keras_tokenizer=tokenizer, max_seq_len=128,\n",
    "                       )\n",
    "\n",
    "val_ds_resnet = CLIPDataset(df_val, base_dir=base_dir, transform=transform_resnet,\n",
    "                     keras_tokenizer=tokenizer, max_seq_len=128,\n",
    "                     )\n",
    "\n",
    "train_loader_resnet = DataLoader(train_ds_resnet, batch_size=64, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True,\n",
    "                          collate_fn=clip_collate)\n",
    "\n",
    "val_loader_resnet = DataLoader(val_ds_resnet, batch_size=64, shuffle=False,\n",
    "                        num_workers=4, pin_memory=True,\n",
    "                        collate_fn=clip_collate)\n",
    "resnet_cls = load_best_resnet18(\n",
    "    num_classes=4,\n",
    "    ckpt_path=\"../notebooks/best-model-resnet.pth\",\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "img_encoder = ImageEncoderFromResNet18(resnet_cls, proj_dim=256, train_backbone=True)\n",
    "\n",
    "text_cls = SmallBERTPourClassification(\n",
    "    vocab_size=1000,      \n",
    "    sequence_length=128,   \n",
    "    embed_dim=128,\n",
    "    num_heads=4,\n",
    "    ff_dim=256,\n",
    "    num_layers=2,\n",
    "    num_classes=4\n",
    ")\n",
    "txt_encoder = TextEncoderFromSmallBERT(text_cls, proj_dim=256)\n",
    "\n",
    "\n",
    "clip_model = CLIPLikeModel(img_encoder, txt_encoder).to(device)\n",
    "\n",
    "def set_trainable(m, flag: bool):\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "\n",
    "set_trainable(clip_model.text_encoder.encoder, False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, clip_model.parameters()),\n",
    "    lr=1e-3, weight_decay=1e-4\n",
    ")\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "fit(clip_model, train_loader_resnet, val_loader_resnet, optimizer, epochs=5, device=device)\n",
    "index_val = build_clip_index(clip_model, val_loader_resnet, device=device)\n",
    "res = show_retrieval_from_text(\n",
    "    clip_model, index_val,\n",
    "    query_text=\"the man playing with a ball\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=64, vocab_size=1000, oov_id=1,\n",
    "    topk=5, device=device, show_images=True\n",
    ")\n",
    "\n",
    "res = show_retrieval_from_image(\n",
    "    clip_model, index_val,\n",
    "    query_image=str(val_ds_resnet._get_img_path_from_idx(121\n",
    ")), \n",
    "    transform=transform_resnet,\n",
    "    topk=5, device=device, show=True\n",
    ")\n",
    "\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58772c8e",
   "metadata": {},
   "source": [
    "### TRAINING DISTILLBERT RESNET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1132ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_resnet = CLIPDataset(df_train, base_dir=base_dir, transform=transform_resnet,\n",
    "                       keras_tokenizer=tokenizer, max_seq_len=128,\n",
    "                       )\n",
    "\n",
    "val_ds_resnet = CLIPDataset(df_val, base_dir=base_dir, transform=transform_resnet,\n",
    "                     keras_tokenizer=tokenizer, max_seq_len=128,\n",
    "                     )\n",
    "\n",
    "train_loader_resnet = DataLoader(train_ds_resnet, batch_size=64, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True,\n",
    "                          collate_fn=clip_collate)\n",
    "\n",
    "val_loader_resnet = DataLoader(val_ds_resnet, batch_size=64, shuffle=False,\n",
    "                        num_workers=4, pin_memory=True,\n",
    "                        collate_fn=clip_collate)\n",
    "resnet_cls = load_best_resnet18(\n",
    "    num_classes=4,\n",
    "    ckpt_path=\"../notebooks/best-model-resnet.pth\",\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "img_encoder = ImageEncoderFromResNet18(resnet_cls, proj_dim=256, train_backbone=True)\n",
    "\n",
    "txt_encoder = TextEncoderFromDistilBERT(distilbert_cls, proj_dim=256)\n",
    "clip_model = CLIPLikeModelHF(img_encoder, txt_encoder, init_temp=0.07).to(device)\n",
    "\n",
    "clip_model = CLIPLikeModel(img_encoder, txt_encoder).to(device)\n",
    "\n",
    "def set_trainable(m, flag: bool):\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "\n",
    "set_trainable(clip_model.image_encoder.backbone, True)\n",
    "set_trainable(clip_model.text_encoder.backbone, True)\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "optimizer = torch.optim.AdamW(clip_model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "fit_hf(clip_model, train_loader, val_loader, optimizer, epochs=10, device=device)\n",
    "res = show_retrieval_from_text(\n",
    "    clip_model, index_val,\n",
    "    query_text=\"the man playing with a ball\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=64, vocab_size=1000, oov_id=1,\n",
    "    topk=5, device=device, show_images=True\n",
    ")\n",
    "\n",
    "res = show_retrieval_from_image(\n",
    "    clip_model, index_val,\n",
    "    query_image=str(val_ds_resnet._get_img_path_from_idx(121\n",
    ")), \n",
    "    transform=transform_resnet,\n",
    "    topk=5, device=device, show=True\n",
    ")\n",
    "index_val = build_clip_index_hf(clip_model, val_loader, device=device)\n",
    "\n",
    "\n",
    "show_retrieval_from_text_hf(\n",
    "    clip_model, index_val,\n",
    "    query_text=\"a man riding a bike\",\n",
    "    tokenizer=hf_tokenizer,\n",
    "    max_len=128,\n",
    "    topk=5,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "show_retrieval_from_image_hf(\n",
    "    clip_model, index_val,\n",
    "    query_image=Image.open(val_ds._get_img_path_from_idx(121)),\n",
    "    transform=transform_resnet,\n",
    "    topk=5,\n",
    "    device=device\n",
    "\n",
    ")           "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
