{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68e04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Tuple\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Optional, Tuple, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import Dict\n",
    "from transformers import DistilBertTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45786d50",
   "metadata": {},
   "source": [
    "### UTILS METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36797db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(filename: str):\n",
    "    return filename.split(\"_\")[0]\n",
    "\n",
    "def get_uuid(filename: str):\n",
    "    name = Path(filename).stem\n",
    "    parts = name.split(\"_\")\n",
    "    return \"_\".join(parts[:2])\n",
    "\n",
    "def build_augmented_path(img_path: Path, base_dir: Path):\n",
    "    img_path = Path(img_path)\n",
    "    filename = img_path.name\n",
    "    label = get_label(filename)\n",
    "    uuid = get_uuid(filename)\n",
    "    uuid_dir = \"_\".join(filename.split(\".\")[0].split(\"_\")[1:])\n",
    "\n",
    "\n",
    "    return base_dir  / uuid / uuid_dir / filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d250473e",
   "metadata": {},
   "source": [
    "### CLIP DATASET BASE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61819d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset unifié pour CLIP supportant:\n",
    "    - Images avec transforms (Visual Classifier)\n",
    "    - Tokenization Keras (SmallBERT)\n",
    "    - Tokenization HuggingFace (DistilBERT)\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame avec colonnes 'image_path', 'label', 'caption'\n",
    "        base_dir: Répertoire de base pour les images\n",
    "        transform: Transformations d'image (torchvision)\n",
    "        keras_tokenizer: Tokenizer Keras (optionnel, pour SmallBERT)\n",
    "        hf_tokenizer: Tokenizer HuggingFace (optionnel, pour DistilBERT)\n",
    "        max_seq_len: Longueur max des séquences\n",
    "        vocab_size: Taille du vocabulaire pour Keras tokenizer\n",
    "        text_mode: 'keras', 'hf', ou 'both' pour le type de tokenization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        base_dir: Path,\n",
    "        transform,\n",
    "        keras_tokenizer=None,\n",
    "        hf_tokenizer=None,\n",
    "        max_seq_len: int = 64,\n",
    "        vocab_size: int = 1000,\n",
    "        text_mode: str = \"keras\"  # 'keras', 'hf'\n",
    "    ):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.transform = transform\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = 1000\n",
    "        self.text_mode = text_mode\n",
    "        \n",
    "        self.img_paths = df[\"image_path\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.captions = df[\"caption\"].tolist()\n",
    "        \n",
    "        self.classes = sorted(set(self.labels))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "        self.num_classes = len(self.classes)\n",
    "        \n",
    "    \n",
    "        self.keras_tokenizer = keras_tokenizer\n",
    "        self.hf_tokenizer = hf_tokenizer\n",
    "        \n",
    "        # Setup OOV pour Keras\n",
    "        if self.keras_tokenizer is not None:\n",
    "            oov_tok = getattr(self.keras_tokenizer, \"oov_token\", \"<OOV>\")\n",
    "            self.oov_id = self.keras_tokenizer.word_index.get(oov_tok, 1)\n",
    "            if self.oov_id is None or self.oov_id >= self.vocab_size:\n",
    "                self.oov_id = 1\n",
    "        \n",
    "        # Validation\n",
    "        self._validate_tokenizers()\n",
    "    \n",
    "    def _validate_tokenizers(self):\n",
    "        \"\"\"Valide que les tokenizers nécessaires sont fournis\"\"\"\n",
    "        if self.text_mode in [\"keras\", \"both\"] and self.keras_tokenizer is None:\n",
    "            raise ValueError(\"keras_tokenizer requis pour text_mode='keras' ou 'both'\")\n",
    "        if self.text_mode in [\"hf\", \"both\"] and self.hf_tokenizer is None:\n",
    "            raise ValueError(\"hf_tokenizer requis pour text_mode='hf' ou 'both'\")\n",
    "    \n",
    "    def _build_image_path(self, img_path: str) -> Path:\n",
    "        \"\"\"Construit le chemin complet de l'image\"\"\"\n",
    "        return Path(build_augmented_path(img_path, self.base_dir))\n",
    "    \n",
    "    def _tokenize_keras(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"Tokenization style SmallBERT (Keras)\"\"\"\n",
    "        seq = self.keras_tokenizer.texts_to_sequences([text])[0]\n",
    "        # Remplace les tokens hors vocab par OOV\n",
    "        seq = [\n",
    "            t if (t is not None and 0 <= t < self.vocab_size) else self.oov_id\n",
    "            for t in seq\n",
    "        ]\n",
    "        # Padding\n",
    "        padded = pad_sequences(\n",
    "            [seq],\n",
    "            maxlen=self.max_seq_len,\n",
    "            padding=\"post\",\n",
    "            truncating=\"post\",\n",
    "            value=0\n",
    "        )[0].astype(np.int64)\n",
    "        \n",
    "        return torch.tensor(padded, dtype=torch.long)\n",
    "    \n",
    "    def _tokenize_hf(self, text: str) -> dict:\n",
    "        \"\"\"Tokenization style DistilBERT (HuggingFace)\"\"\"\n",
    "        enc = self.hf_tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_seq_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0).to(torch.long),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0).to(torch.long)\n",
    "        }\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        # === IMAGE ===\n",
    "        img_path = self._build_image_path(self.img_paths[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # === CAPTION ===\n",
    "        caption = str(self.captions[idx])\n",
    "        \n",
    "        # === LABEL ===\n",
    "        label_str = self.labels[idx]\n",
    "        label = torch.tensor(self.class_to_idx[label_str], dtype=torch.long)\n",
    "        \n",
    "        # === OUTPUT DICT ===\n",
    "        output = {\n",
    "            \"idx\": torch.tensor(idx, dtype=torch.long),\n",
    "            \"image\": img,\n",
    "            \"label\": label,\n",
    "            \"caption\": caption,\n",
    "        }\n",
    "        \n",
    "        # Tokenization selon le mode\n",
    "        if self.text_mode in [\"keras\", \"both\"]:\n",
    "            input_ids_keras = self._tokenize_keras(caption)\n",
    "            output[\"input_ids_keras\"] = input_ids_keras\n",
    "            output[\"attention_mask_keras\"] = (input_ids_keras != 0).long()\n",
    "            \n",
    "            if self.text_mode == \"keras\":\n",
    "                output[\"input_ids\"] = input_ids_keras\n",
    "                output[\"attention_mask\"] = output[\"attention_mask_keras\"]\n",
    "        \n",
    "        if self.text_mode == \"hf\":\n",
    "            hf_tokens = self._tokenize_hf(caption)\n",
    "            output[\"input_ids_hf\"] = hf_tokens[\"input_ids\"]\n",
    "            output[\"attention_mask_hf\"] = hf_tokens[\"attention_mask\"]\n",
    "            \n",
    "            if self.text_mode == \"hf\":\n",
    "                output[\"input_ids\"] = hf_tokens[\"input_ids\"]\n",
    "                output[\"attention_mask\"] = hf_tokens[\"attention_mask\"]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    \n",
    "    def _get_img_path_from_idx(self, idx: int) -> Path:\n",
    "        return self._build_image_path(self.img_paths[idx])\n",
    "    \n",
    "    def _get_caption_from_idx(self, idx: int) -> str:\n",
    "        return self.captions[idx]\n",
    "    \n",
    "    def _get_label_from_idx(self, idx: int) -> str:\n",
    "        return self.labels[idx]\n",
    "    \n",
    "    def _get_img_size(self, idx: int) -> Tuple[int, int]:\n",
    "        img_path = self._build_image_path(self.img_paths[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            for t in self.transform.transforms:\n",
    "                if isinstance(t, transforms.Resize):\n",
    "                    img = t(img)\n",
    "        return img.height, img.width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df153002",
   "metadata": {},
   "source": [
    "### VISION ENCODER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf28d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBasicEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, projection_dim: int = 256, input_size: Tuple[int, int] = (224, 224)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # DROPOUT FEATUREMAPS\n",
    "        self.dropout = nn.Dropout2d(p=0.2)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # MISE EN ESPACE COMMUN\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(256, projection_dim),\n",
    "            nn.LayerNorm(projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, 3, H, W) images\n",
    "        Returns:\n",
    "            embeddings: (B, projection_dim) \n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.gap(x).flatten(1)  # (B, 256)\n",
    "        \n",
    "        # Projection L2 normalization\n",
    "        embeddings = self.projection(x)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=-1)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec8d86",
   "metadata": {},
   "source": [
    "### RESNET ENCODER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f95041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "class ResNet18Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet18 adapté pour CLIP   avec les poids d'ImageNet\n",
    "    freeze_mode = cf Visual Classifier\n",
    "    \"\"\"\n",
    "    def __init__(self, projection_dim: int = 256, freeze_mode: str = \"none\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "        resnet = resnet18(weights=weights)\n",
    "        \n",
    "        self.conv1 = resnet.conv1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        \n",
    "        self.feature_dim = 512\n",
    "        \n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Projection  CLIP\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, projection_dim),\n",
    "            nn.LayerNorm(projection_dim)\n",
    "        )\n",
    "        \n",
    "        # Appliquer le freeze selon le mode\n",
    "        self.freeze_mode = freeze_mode\n",
    "        self._apply_freeze_mode(freeze_mode)\n",
    "    \n",
    "    def _apply_freeze_mode(self, mode: str):\n",
    "        if mode == \"none\":\n",
    "            pass\n",
    "        elif mode == \"all\":\n",
    "            for name, param in self.named_parameters():\n",
    "                if \"projection\" not in name:\n",
    "                    param.requires_grad = False\n",
    "        elif mode == \"except_layer4\":\n",
    "            for param in self.conv1.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.bn1.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.layer1.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.layer2.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.layer3.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "        elif mode == \"except_layer3_4\":\n",
    "            for param in self.conv1.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.bn1.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.layer1.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.layer2.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            raise ValueError(f\"freeze_mode inconnu: {mode}\")\n",
    "    \n",
    "    def unfreeze_all(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def get_trainable_params_info(self):\n",
    "        total = sum(p.numel() for p in self.parameters())\n",
    "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"ResNet18Encoder - freeze_mode='{self.freeze_mode}'\")\n",
    "        print(f\"  Total params: {total:,}\")\n",
    "        print(f\"  Trainable: {trainable:,} ({100*trainable/total:.1f}%)\")\n",
    "        print(f\"  Frozen: {total-trainable:,} ({100*(total-trainable)/total:.1f}%)\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.gap(x).flatten(1)\n",
    "        \n",
    "        embeddings = self.projection(x)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=-1)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e6ec4",
   "metadata": {},
   "source": [
    "### SMALLBERT TEXT ENCODER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478bd7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, sequence_length: int, vocab_size: int, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(sequence_length, embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len = x.size()\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        return self.token_embeddings(x) + self.position_embeddings(positions)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, padding_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        attn_output, _ = self.att(x, x, x, key_padding_mask=padding_mask)\n",
    "        x = self.layernorm1(x + self.dropout1(attn_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        return self.layernorm2(x + self.dropout2(ffn_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7495d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallBERTEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_length: int,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int = 256,\n",
    "        num_heads: int = 4,\n",
    "        ff_dim: int = 512,\n",
    "        num_layers: int = 2,\n",
    "        projection_dim: int = 256\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = PositionalEmbedding(sequence_length, vocab_size, embed_dim)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.pooler = nn.Linear(embed_dim, 1)\n",
    "        \n",
    "        # Proje CLIP\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embed_dim, projection_dim),\n",
    "            nn.LayerNorm(projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: (B, T) token ids, 0 = PAD\n",
    "        Returns:\n",
    "            embeddings: (B, projection_dim) \n",
    "        \"\"\"\n",
    "        padding_mask = (input_ids == 0)\n",
    "        x = self.pos_embedding(input_ids)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x, padding_mask=padding_mask)\n",
    "\n",
    "        x = self.layernorm(x)  #(B, T, embed_dim)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        scores = self.pooler(x).squeeze(-1)  #(B, T)\n",
    "        scores = scores.masked_fill(padding_mask, float(\"-inf\"))\n",
    "        weights = torch.softmax(scores, dim=1)  #B, T)\n",
    "        pooled = torch.sum(x * weights.unsqueeze(-1), dim=1)  # (B, embed_dim)\n",
    "        \n",
    "        embeddings = self.projection(pooled)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=-1)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1bcfdf",
   "metadata": {},
   "source": [
    "### CLIP LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c433531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, temperature: float = 0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = nn.Parameter(torch.tensor(temperature))\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        image_embeddings: torch.Tensor, \n",
    "        text_embeddings: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_embeddings: (B, D)\n",
    "            text_embeddings: (B, D)\n",
    "        Returns:\n",
    "            loss: contrastive loss\n",
    "        \"\"\"\n",
    "        #Calcul de la similarité cosinus (les embeddings sont déjà normalisés)\n",
    "        #logits[i, j] = similarité entre image_i et text_j\n",
    "        logits = (image_embeddings @ text_embeddings.T) / self.temperature  # (B, B)\n",
    "        \n",
    "        #la diagonale (image_i correspond à text_i)\n",
    "        batch_size = image_embeddings.size(0)\n",
    "        labels = torch.arange(batch_size, device=logits.device)\n",
    "        \n",
    "        #Cross-entropy sym\n",
    "        loss_i2t = F.cross_entropy(logits, labels)      # image -> text\n",
    "        loss_t2i = F.cross_entropy(logits.T, labels)    # text -> image\n",
    "        \n",
    "        loss = (loss_i2t + loss_t2i) / 2.0\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4391aa",
   "metadata": {},
   "source": [
    "### CONTRASTIVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a86e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MiniCLIP(nn.Module):\n",
    "    \"\"\"\n",
    "    Mini-CLIP: Modèle contrastif image-texte\n",
    "    Combine CNNBasic (vision) et SmallBERT (texte)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Text encoder params\n",
    "        sequence_length: int = 64,\n",
    "        vocab_size: int = 5000,\n",
    "        text_embed_dim: int = 256,\n",
    "        text_num_heads: int = 4,\n",
    "        text_ff_dim: int = 512,\n",
    "        text_num_layers: int = 2,\n",
    "        # Shared params\n",
    "        projection_dim: int = 256,\n",
    "        temperature: float = 0.07\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Encodeurs\n",
    "        self.image_encoder = CNNBasicEncoder(projection_dim=projection_dim)\n",
    "        self.text_encoder = SmallBERTEncoder(\n",
    "            sequence_length=sequence_length,\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=text_embed_dim,\n",
    "            num_heads=text_num_heads,\n",
    "            ff_dim=text_ff_dim,\n",
    "            num_layers=text_num_layers,\n",
    "            projection_dim=projection_dim\n",
    "        )\n",
    "        \n",
    "        #Loss\n",
    "        self.clip_loss = CLIPLoss(temperature=temperature)\n",
    "        \n",
    "        #Paramètre de température (learnable)\n",
    "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1.0 / temperature)))\n",
    "\n",
    "    def encode_image(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        return self.image_encoder(images)\n",
    "    \n",
    "    def encode_text(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        return self.text_encoder(input_ids)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        images: torch.Tensor, \n",
    "        input_ids: torch.Tensor\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        image_embeddings = self.encode_image(images)\n",
    "        text_embeddings = self.encode_text(input_ids)\n",
    "        \n",
    "        # similarity matrix\n",
    "        logit_scale = self.logit_scale.exp().clamp(max=100)\n",
    "        logits = logit_scale * (image_embeddings @ text_embeddings.T)\n",
    "        \n",
    "        #Loss\n",
    "        loss = self.clip_loss(image_embeddings, text_embeddings)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"image_embeddings\": image_embeddings,\n",
    "            \"text_embeddings\": text_embeddings,\n",
    "            \"logits\": logits,\n",
    "            \"logit_scale\": logit_scale\n",
    "        }\n",
    "    \n",
    "    def get_similarity(\n",
    "        self, \n",
    "        images: torch.Tensor, \n",
    "        input_ids: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Calcule la matrice de similarité image-texte\"\"\"\n",
    "        image_emb = self.encode_image(images)\n",
    "        text_emb = self.encode_text(input_ids)\n",
    "        logit_scale = self.logit_scale.exp().clamp(max=100)\n",
    "        return logit_scale * (image_emb @ text_emb.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63faa8a",
   "metadata": {},
   "source": [
    "### CONTRASTIVE RESNET MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f00d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniCLIPResnet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_length: int = 64,\n",
    "        vocab_size: int = 5000,\n",
    "        text_embed_dim: int = 256,\n",
    "        text_num_heads: int = 4,\n",
    "        text_ff_dim: int = 512,\n",
    "        text_num_layers: int = 2,\n",
    "        projection_dim: int = 256,\n",
    "        temperature: float = 0.07,\n",
    "        freeze_mode: str = \"except_layer4\"  \n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_encoder = ResNet18Encoder(\n",
    "            projection_dim=projection_dim,\n",
    "            freeze_mode=freeze_mode\n",
    "        )\n",
    "        \n",
    "        self.text_encoder = SmallBERTEncoder(\n",
    "            sequence_length=sequence_length,\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=text_embed_dim,\n",
    "            num_heads=text_num_heads,\n",
    "            ff_dim=text_ff_dim,\n",
    "            num_layers=text_num_layers,\n",
    "            projection_dim=projection_dim\n",
    "        )\n",
    "        \n",
    "        self.clip_loss = CLIPLoss(temperature=temperature)\n",
    "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1.0 / temperature)))\n",
    "\n",
    "    def encode_image(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        return self.image_encoder(images)\n",
    "    \n",
    "    def encode_text(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        return self.text_encoder(input_ids)\n",
    "    \n",
    "    def forward(self, images: torch.Tensor, input_ids: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        image_embeddings = self.encode_image(images)\n",
    "        text_embeddings = self.encode_text(input_ids)\n",
    "        \n",
    "        logit_scale = self.logit_scale.exp().clamp(max=100)\n",
    "        logits = logit_scale * (image_embeddings @ text_embeddings.T)\n",
    "        loss = self.clip_loss(image_embeddings, text_embeddings)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"image_embeddings\": image_embeddings,\n",
    "            \"text_embeddings\": text_embeddings,\n",
    "            \"logits\": logits,\n",
    "            \"logit_scale\": logit_scale\n",
    "        }\n",
    "    \n",
    "    def get_similarity(self, images: torch.Tensor, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        image_emb = self.encode_image(images)\n",
    "        text_emb = self.encode_text(input_ids)\n",
    "        logit_scale = self.logit_scale.exp().clamp(max=100)\n",
    "        return logit_scale * (image_emb @ text_emb.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b75069",
   "metadata": {},
   "source": [
    "### TRAINING METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed274d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch_clip(\n",
    "    model: MiniCLIP,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    epochs: int\n",
    ") -> float:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    loop = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs} [TRAIN]\")\n",
    "    for batch in loop:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)   #utilise input_ids_keras si mode keras\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        outputs = model(images, input_ids)\n",
    "        loss = outputs[\"loss\"]\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        loop.set_postfix(\n",
    "            loss=running_loss / (loop.n + 1),\n",
    "            temp=outputs[\"logit_scale\"].item()\n",
    "        )\n",
    "\n",
    "    return running_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_clip(\n",
    "    model: MiniCLIP,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    epoch: int,\n",
    "    epochs: int\n",
    ") -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    all_image_emb = []\n",
    "    all_text_emb = []\n",
    "    all_labels = []\n",
    "\n",
    "    loop = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs} [EVAL]\")\n",
    "    for batch in loop:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"label\"]\n",
    "\n",
    "        outputs = model(images, input_ids)\n",
    "        running_loss += outputs[\"loss\"].item()\n",
    "        \n",
    "        all_image_emb.append(outputs[\"image_embeddings\"].cpu())\n",
    "        all_text_emb.append(outputs[\"text_embeddings\"].cpu())\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    \n",
    "    # Calcul des métriques de retrieval\n",
    "    all_image_emb = torch.cat(all_image_emb)\n",
    "    all_text_emb = torch.cat(all_text_emb)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    \n",
    "    # Image-to-Text retrieval (R@1, R@5)\n",
    "    similarity = all_image_emb @ all_text_emb.T\n",
    "    i2t_ranks = (similarity.argsort(dim=1, descending=True) == torch.arange(len(similarity)).unsqueeze(1)).float().argmax(dim=1)\n",
    "    i2t_r1 = (i2t_ranks < 1).float().mean().item()\n",
    "    i2t_r5 = (i2t_ranks < 5).float().mean().item()\n",
    "    \n",
    "    # Text-to-Image retriveal\n",
    "    t2i_ranks = (similarity.T.argsort(dim=1, descending=True) == torch.arange(len(similarity)).unsqueeze(1)).float().argmax(dim=1)\n",
    "    t2i_r1 = (t2i_ranks < 1).float().mean().item()\n",
    "    t2i_r5 = (t2i_ranks < 5).float().mean().item()\n",
    "    \n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"i2t_r1\": i2t_r1,\n",
    "        \"i2t_r5\": i2t_r5,\n",
    "        \"t2i_r1\": t2i_r1,\n",
    "        \"t2i_r5\": t2i_r5,\n",
    "        \"mean_r1\": (i2t_r1 + t2i_r1) / 2\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a32956",
   "metadata": {},
   "source": [
    "### RETRIEVAL IMAGES FROM TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1193ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def retrieve_images_from_text(model, text_query, dataset, keras_tokenizer, device, top_k=5):\n",
    "    model.eval()\n",
    "    \n",
    "    seq = keras_tokenizer.texts_to_sequences([text_query])[0]\n",
    "    padded = pad_sequences([seq], maxlen=64, padding=\"post\", truncating=\"post\")[0]\n",
    "    input_ids = torch.tensor(padded, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    text_emb = model.encode_text(input_ids)\n",
    "    \n",
    "    all_img_embs = []\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    for batch in loader:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        img_emb = model.encode_image(images)\n",
    "        all_img_embs.append(img_emb.cpu())\n",
    "    \n",
    "    all_img_embs = torch.cat(all_img_embs)\n",
    "    \n",
    "    similarity = (text_emb.cpu() @ all_img_embs.T).squeeze(0)\n",
    "    top_indices = similarity.argsort(descending=True)[:top_k]\n",
    "    \n",
    "    return top_indices.tolist(), similarity[top_indices].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f4b80",
   "metadata": {},
   "source": [
    "### RETRIEVAL TEXT FROM IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c83674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "@torch.no_grad()\n",
    "def retrieve_texts_from_image_simple(\n",
    "    model,\n",
    "    image_tensor,     \n",
    "    dataset,\n",
    "    device,\n",
    "    top_k=5,\n",
    "    batch_size=32\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    img = image_tensor.unsqueeze(0).to(device)   #(1,3,H,W)\n",
    "    img_emb = model.encode_image(img).cpu()      (1,D)\n",
    "\n",
    "    all_txt_embs = []\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for batch in loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        txt_emb = model.encode_text(input_ids)\n",
    "        all_txt_embs.append(txt_emb.cpu())\n",
    "\n",
    "    all_txt_embs = torch.cat(all_txt_embs, dim=0)  #(N,D)\n",
    "\n",
    "    similarity = (img_emb @ all_txt_embs.T).squeeze(0)  # (N,)\n",
    "    top_indices = similarity.argsort(descending=True)[:top_k]\n",
    "\n",
    "    return top_indices.tolist(), similarity[top_indices].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4762dd4",
   "metadata": {},
   "source": [
    "### TRANSFORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fb7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_base = transforms.Compose(\n",
    "    [transforms.Resize((300, 500)),\n",
    "        transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cccadbc",
   "metadata": {},
   "source": [
    "### VISUALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_all_embeddings(model, loader, device):\n",
    "    model.eval()\n",
    "    img_embs, txt_embs, labels, captions = [], [], [], []\n",
    "    \n",
    "    for batch in loader:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        \n",
    "        img_emb = model.encode_image(images).cpu()\n",
    "        txt_emb = model.encode_text(input_ids).cpu()\n",
    "        \n",
    "        img_embs.append(img_emb)\n",
    "        txt_embs.append(txt_emb)\n",
    "        labels.append(batch[\"label\"])\n",
    "        captions.extend(batch[\"caption\"])\n",
    "    \n",
    "    return {\n",
    "        \"image\": torch.cat(img_embs).numpy(),\n",
    "        \"text\": torch.cat(txt_embs).numpy(),\n",
    "        \"labels\": torch.cat(labels).numpy(),\n",
    "        \"captions\": captions\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b90103",
   "metadata": {},
   "source": [
    "### DATASET LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0162ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = Path(\"../data/final_dataset_noaug2/metadata.csv\")\n",
    "metadata_noaug_path = Path(\"../data/final_dataset/metadata.csv\")\n",
    "base_dir = Path(\"../data/final_dataset\")\n",
    "base_dir_noaug = Path(\"../data/final_dataset_noaug2\")\n",
    "df = pd.read_csv(metadata_path)\n",
    "df_noaug = pd.read_csv(metadata_noaug_path)\n",
    "\n",
    "print(df.columns)\n",
    "print(df.iloc[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2691f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.drop_duplicates(subset=\"caption\").reset_index(drop=True)\n",
    "\n",
    "keras_tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "keras_tokenizer.fit_on_texts(df_clean[\"caption\"])\n",
    "\n",
    "hf_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c714f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_smallbert = CLIPDataset(\n",
    "    df=df,\n",
    "    base_dir=base_dir,\n",
    "    transform=transform,\n",
    "    keras_tokenizer=keras_tokenizer,\n",
    "    max_seq_len=64,\n",
    "    vocab_size=5000,\n",
    "    text_mode=\"keras\"\n",
    ")\n",
    "\n",
    "dataset_distilbert = CLIPDataset(\n",
    "    df=df,\n",
    "    base_dir=base_dir,\n",
    "    transform=transform,\n",
    "    hf_tokenizer=hf_tokenizer,\n",
    "    max_seq_len=64,\n",
    "    text_mode=\"hf\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f56c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "\n",
    "df_train, df_temp = train_test_split(df, test_size=0.3, random_state=11, stratify=df[\"label\"])\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=11, stratify=df_temp[\"label\"])\n",
    "\n",
    "\n",
    "vocab_size = 5000\n",
    "seq_len = 64\n",
    "keras_tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "keras_tokenizer.fit_on_texts(df_clean[\"caption\"])\n",
    "actual_vocab_size = min(len(keras_tokenizer.word_index) + 1, vocab_size)\n",
    "\n",
    "train_dataset = CLIPDataset(\n",
    "    df=df_train, base_dir=base_dir, transform=transform_base,\n",
    "    keras_tokenizer=keras_tokenizer, max_seq_len=seq_len,\n",
    "    vocab_size=vocab_size, text_mode=\"keras\"\n",
    ")\n",
    "val_dataset = CLIPDataset(\n",
    "    df=df_val, base_dir=base_dir, transform=transform_base,\n",
    "    keras_tokenizer=keras_tokenizer, max_seq_len=seq_len,\n",
    "    vocab_size=vocab_size, text_mode=\"keras\"\n",
    ")\n",
    "train_dataset_resnet= CLIPDataset(\n",
    "    df=df_train, base_dir=base_dir, transform=transform,\n",
    "    keras_tokenizer=keras_tokenizer, max_seq_len=seq_len,\n",
    "    vocab_size=vocab_size, text_mode=\"keras\"\n",
    ")\n",
    "val_dataset_resnet = CLIPDataset(\n",
    "    df=df_val, base_dir=base_dir, transform=transform,\n",
    "    keras_tokenizer=keras_tokenizer, max_seq_len=seq_len,\n",
    "    vocab_size=vocab_size, text_mode=\"hf\"\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "train_loader_resnet = DataLoader(train_dataset_resnet, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader_resnet= DataLoader(val_dataset_resnet, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b3016",
   "metadata": {},
   "source": [
    "### TRAINING WITH CNN SMALLBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a665bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniCLIP(\n",
    "    sequence_length=seq_len,\n",
    "    vocab_size=actual_vocab_size,\n",
    "    text_embed_dim=256,\n",
    "    text_num_heads=4,\n",
    "    text_ff_dim=512,\n",
    "    text_num_layers=2,\n",
    "    projection_dim=256,\n",
    "    temperature=0.07\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    lr=1e-4, \n",
    "    weight_decay=0.01\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "epochs = 20\n",
    "best_loss = float(\"inf\")\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"val_r1\": []}\n",
    "patience = 5\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_one_epoch_clip(model, train_loader, optimizer, device, epoch, epochs)\n",
    "    val_metrics = evaluate_clip(model, val_loader, device, epoch, epochs)\n",
    "    \n",
    "    if val_metrics[\"loss\"] < best_loss:\n",
    "        best_loss = val_metrics[\"loss\"]\n",
    "        torch.save(model.state_dict(), \"../models/best_miniclip_cnn_smallbert.pth\")\n",
    "        no_improve = 0\n",
    "        print(\" Saved best model\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\" Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "    history[\"val_r1\"].append(val_metrics[\"mean_r1\"])\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_metrics['loss']:.4f}\")\n",
    "    print(f\"  I2T R@1: {val_metrics['i2t_r1']*100:.1f}% | R@5: {val_metrics['i2t_r5']*100:.1f}%\")\n",
    "    print(f\"  T2I R@1: {val_metrics['t2i_r1']*100:.1f}% | R@5: {val_metrics['t2i_r5']*100:.1f}%\")\n",
    "    \n",
    "    if val_metrics[\"loss\"] < best_loss:\n",
    "        best_loss = val_metrics[\"loss\"]\n",
    "        torch.save(model.state_dict(), \"../models/best_miniclip_cnn_smallbert.pth\")\n",
    "        print(\" Saved best model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea17580",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_all_embeddings(model, val_loader, device)\n",
    "\n",
    "combined = np.vstack([embeddings[\"image\"], embeddings[\"text\"]])\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "reduced = tsne.fit_transform(combined)\n",
    "\n",
    "n = len(embeddings[\"image\"])\n",
    "img_2d = reduced[:n]\n",
    "txt_2d = reduced[n:]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "colors_modal = ['blue'] * n + ['red'] * n\n",
    "axes[0].scatter(reduced[:n, 0], reduced[:n, 1], c='blue', alpha=0.5, label='Images', s=20)\n",
    "axes[0].scatter(reduced[n:, 0], reduced[n:, 1], c='red', alpha=0.5, label='Textes', s=20)\n",
    "axes[0].set_title(\"Espace d'embedding par modalité\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "scatter = axes[1].scatter(img_2d[:, 0], img_2d[:, 1], c=embeddings[\"labels\"], \n",
    "                          cmap='tab10', alpha=0.7, s=30)\n",
    "axes[1].set_title(\"Embeddings images par classe\")\n",
    "cbar = plt.colorbar(scatter, ax=axes[1])\n",
    "cbar.set_ticks(range(len(class_names)))\n",
    "cbar.set_ticklabels(class_names)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4013643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"a dog playing\", \"a red bike\", \"a ball on grass\", \"water and waves\", \"a dog in the park\", \"the girl is riding her bike\"]\n",
    "\n",
    "fig, axes = plt.subplots(len(queries), 6, figsize=(18, 4*len(queries)))\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    indices, scores = retrieve_images_from_text(\n",
    "       model, query, val_dataset, keras_tokenizer, device, top_k=5\n",
    "    )\n",
    "    \n",
    "    axes[i, 0].text(0.5, 0.5, f\"Query:\\n'{query}'\", ha='center', va='center', fontsize=12)\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    for j, (idx, score) in enumerate(zip(indices, scores)):\n",
    "        img_path = val_dataset._get_img_path_from_idx(idx)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        axes[i, j+1].imshow(img)\n",
    "        axes[i, j+1].set_title(f\"Score: {score:.3f}\\n{val_dataset._get_label_from_idx(idx)}\")\n",
    "        axes[i, j+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c1455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.randint(0, len(val_dataset) - 1)\n",
    "sample = val_dataset[idx]\n",
    "\n",
    "query_image = sample[\"image\"]\n",
    "\n",
    "indices, scores = retrieve_texts_from_image_simple(\n",
    "    model,\n",
    "    query_image,\n",
    "    val_dataset,\n",
    "    device,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "def denormalize_cnn(x):\n",
    "    mean = torch.tensor([0.5, 0.5, 0.5]).view(3,1,1)\n",
    "    std = torch.tensor([0.5, 0.5, 0.5]).view(3,1,1)\n",
    "    return (x * std + mean).clamp(0,1)\n",
    "\n",
    "\n",
    "plt.imshow(denormalize_cnn(query_image).permute(1,2,0))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Query image\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTOP-5 phrases les plus proches :\\n\")\n",
    "\n",
    "for rank, (idx, score) in enumerate(zip(indices, scores), start=1):\n",
    "    if hasattr(val_dataset, \"_get_caption_from_idx\"):\n",
    "        caption = val_dataset._get_caption_from_idx(idx)\n",
    "    elif hasattr(val_dataset, \"df\") and \"caption\" in val_dataset.df.columns:\n",
    "        caption = val_dataset.df.iloc[idx][\"caption\"]\n",
    "    else:\n",
    "        caption = f\"(caption idx={idx} introuvable)\"\n",
    "\n",
    "    print(f\"{rank}. [{score:.3f}] {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cff9ff3",
   "metadata": {},
   "source": [
    "### TRAINING WITH RESNET SMALLBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc81a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniCLIPResnet(\n",
    "    sequence_length=seq_len,\n",
    "    vocab_size=actual_vocab_size,\n",
    "    text_embed_dim=384,\n",
    "    text_num_heads=6,\n",
    "    text_ff_dim=768,\n",
    "    text_num_layers=3,\n",
    "    projection_dim=384,\n",
    "    temperature=0.05,\n",
    "    freeze_mode=\"except_layer3_4\" \n",
    ").to(device)\n",
    "\n",
    "model.image_encoder.get_trainable_params_info()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=5e-4,\n",
    "    weight_decay=0.02\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=5, T_mult=2\n",
    ")\n",
    "epochs = 20\n",
    "best_loss = float(\"inf\")\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"val_r1\": []}\n",
    "patience = 5\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_one_epoch_clip(model, train_loader_resnet, optimizer, device, epoch, epochs)\n",
    "    val_metrics = evaluate_clip(model, val_loader_resnet, device, epoch, epochs)\n",
    "    \n",
    "    if val_metrics[\"loss\"] < best_loss:\n",
    "        best_loss = val_metrics[\"loss\"]\n",
    "        torch.save(model.state_dict(), \"../models/best_miniclip_resnet_smallbert.pth\")\n",
    "        no_improve = 0\n",
    "        print(\" Saved best model\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\" Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
    "    history[\"val_r1\"].append(val_metrics[\"mean_r1\"])\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_metrics['loss']:.4f}\")\n",
    "    print(f\"  I2T R@1: {val_metrics['i2t_r1']*100:.1f}% | R@5: {val_metrics['i2t_r5']*100:.1f}%\")\n",
    "    print(f\"  T2I R@1: {val_metrics['t2i_r1']*100:.1f}% | R@5: {val_metrics['t2i_r5']*100:.1f}%\")\n",
    "    \n",
    "    if val_metrics[\"loss\"] < best_loss:\n",
    "        best_loss = val_metrics[\"loss\"]\n",
    "        torch.save(model.state_dict(), \"../models/best_miniclip_resnet_smallbert.pth\")\n",
    "        print(\" Saved best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdb4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_all_embeddings(model, val_loader_resnet, device)\n",
    "\n",
    "combined = np.vstack([embeddings[\"image\"], embeddings[\"text\"]])\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "reduced = tsne.fit_transform(combined)\n",
    "\n",
    "n = len(embeddings[\"image\"])\n",
    "img_2d = reduced[:n]\n",
    "txt_2d = reduced[n:]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "colors_modal = ['blue'] * n + ['red'] * n\n",
    "axes[0].scatter(reduced[:n, 0], reduced[:n, 1], c='blue', alpha=0.5, label='Images', s=20)\n",
    "axes[0].scatter(reduced[n:, 0], reduced[n:, 1], c='red', alpha=0.5, label='Textes', s=20)\n",
    "axes[0].set_title(\"Espace d'embedding par modalité\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "scatter = axes[1].scatter(img_2d[:, 0], img_2d[:, 1], c=embeddings[\"labels\"], \n",
    "                          cmap='tab10', alpha=0.7, s=30)\n",
    "axes[1].set_title(\"Embeddings images par classe\")\n",
    "cbar = plt.colorbar(scatter, ax=axes[1])\n",
    "cbar.set_ticks(range(len(class_names)))\n",
    "cbar.set_ticklabels(class_names)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1a1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"a dog playing\", \"a red bike\", \"a ball on grass\", \"water and waves\", \"a dog in the park\", \"the girl is riding her bike\"]\n",
    "\n",
    "fig, axes = plt.subplots(len(queries), 6, figsize=(18, 4*len(queries)))\n",
    "\n",
    "for i, query in enumerate(queries):\n",
    "    indices, scores = retrieve_images_from_text(\n",
    "       model, query, val_dataset_resnet, keras_tokenizer, device, top_k=5\n",
    "    )\n",
    "    \n",
    "    axes[i, 0].text(0.5, 0.5, f\"Query:\\n'{query}'\", ha='center', va='center', fontsize=12)\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    for j, (idx, score) in enumerate(zip(indices, scores)):\n",
    "        img_path = val_dataset_resnet._get_img_path_from_idx(idx)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        axes[i, j+1].imshow(img)\n",
    "        axes[i, j+1].set_title(f\"Score: {score:.3f}\\n{val_dataset._get_label_from_idx(idx)}\")\n",
    "        axes[i, j+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570e7d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.randint(0, len(val_dataset_resnet) - 1)\n",
    "sample = val_dataset_resnet[idx]\n",
    "\n",
    "query_image = sample[\"image\"]\n",
    "\n",
    "indices, scores = retrieve_texts_from_image_simple(\n",
    "    model,\n",
    "    query_image,\n",
    "    val_dataset_resnet,\n",
    "    device,\n",
    "    top_k=5\n",
    ")\n",
    "def denormalize_resnet(x):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "    return (x * std + mean).clamp(0,1)\n",
    "\n",
    "plt.imshow(denormalize_resnet(query_image).permute(1,2,0))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Query image\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTOP-5 phrases les plus proches :\\n\")\n",
    "\n",
    "for rank, (idx, score) in enumerate(zip(indices, scores), start=1):\n",
    "    if hasattr(val_dataset_resnet, \"_get_caption_from_idx\"):\n",
    "        caption = val_dataset_resnet._get_caption_from_idx(idx)\n",
    "    elif hasattr(val_dataset_resnet, \"df\") and \"caption\" in val_dataset_resnet.df.columns:\n",
    "        caption = val_dataset_resnet.df.iloc[idx][\"caption\"]\n",
    "    else:\n",
    "        caption = f\"(caption idx={idx} introuvable)\"\n",
    "\n",
    "    print(f\"{rank}. [{score:.3f}] {caption}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
